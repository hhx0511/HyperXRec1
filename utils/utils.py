'''
    utlls.py: tool class
'''
import os
import re#æ­£åˆ™è¡¨è¾¾å¼åº“ï¼Œç”¨äºæ–‡æœ¬åŒ¹é…ä¸æ›¿æ¢ã€‚
import torch
from collections import Counter#ç»Ÿè®¡å®¹å™¨ä¸­æ¯ä¸ªå…ƒç´ çš„å‡ºç°æ¬¡æ•°ã€‚
from sklearn.manifold import TSNE#t-SNE ç®—æ³•ï¼Œç”¨äºæŠŠé«˜ç»´æ•°æ®é™åˆ° 2D æˆ– 3Dï¼Œå¯è§†åŒ–èšç±»ç»“æœã€‚
import matplotlib.pyplot as plt
from matplotlib.cm import get_cmap
import numpy as np
from typing import Optional#æä¾›ç±»å‹æç¤ºåŠŸèƒ½ã€‚
from transformers import PreTrainedTokenizerBase#HuggingFace ä¸­ tokenizer çš„åŸºç±»
from datasets import Dataset as HFDataset#HuggingFace çš„æ•°æ®é›†æ ¼å¼ï¼Œå…¼å®¹ Trainerï¼Œæ”¯æŒæ•°æ®å¤„ç†ã€ç¼“å­˜ã€åˆ‡åˆ†ç­‰ã€‚

from transformers.trainer_utils import EvalLoopOutput#Trainer ä¸­è¯„ä¼°è¾“å‡ºçš„æ ‡å‡†ç»“æ„ã€‚
from transformers import Trainer#HuggingFace çš„é€šç”¨è®­ç»ƒç±»ï¼Œå°è£…äº†è®­ç»ƒã€è¯„ä¼°ã€ä¿å­˜ã€æ—¥å¿—è®°å½•ç­‰æµç¨‹ã€‚
from transformers.utils import logging#HuggingFace æä¾›çš„æ—¥å¿—è®°å½•æ¨¡å—ã€‚
from torch.utils.data import SequentialSampler#é¡ºåºé‡‡æ ·å™¨ï¼Œç”¨äºæŒ‰é¡ºåºåŠ è½½æ•°æ®è€Œä¸æ˜¯æ‰“ä¹±ã€‚

'''
    process_explain_data_fun: 
        args:
            examples: single data
        padã€tokenizeã€add bos eos token
'''
# ---------------------------------------------
# å°† PyTorch æ•°æ®é›†è½¬æ¢ä¸º HuggingFace æ•°æ®é›†æ ¼å¼ï¼Œæ–¹ä¾¿ä½¿ç”¨ Trainer
# ---------------------------------------------
def TorchDataset2HuggingfaceDataset(torch_dataset, cache_dir = None):
    generator = lambda: (sample for sample in torch_dataset)  # ä½¿ç”¨ç”Ÿæˆå™¨å°è£…åŸå§‹ PyTorch æ•°æ® 
    return HFDataset.from_generator(generator, cache_dir=cache_dir)
# ---------------------------------------------
# æ–‡æœ¬å¤„ç†å‡½æ•°ï¼Œç”¨äºå•æ¡æ ·æœ¬ã€‚å¢åŠ  BOS/EOS tokenï¼Œä¿ç•™ç”¨æˆ·ã€ç‰©å“ä¿¡æ¯
# ---------------------------------------------
def process_fun(examples):
    # examples['text'] = '<>'
    # å¯¹è¾“å…¥çš„æ–‡æœ¬å­—æ®µè¿›è¡Œ tokenizer ç¼–ç ï¼ˆæˆªæ–­æœ€å¤§é•¿åº¦ä¸º 20ï¼‰
    encode_inputs = tokenizer(examples['text'], max_length = 20, truncation = True)
    # æ‰‹åŠ¨ä¿ç•™ user å’Œ item å­—æ®µï¼Œç”¨äºä¸ªæ€§åŒ–æ¨è
    encode_inputs["user"] = examples["user"] # æ·»åŠ ç”¨æˆ·ä¿¡æ¯
    encode_inputs["item"] = examples["item"] # æ·»åŠ ç‰©å“ä¿¡æ¯
    # encode_inputs["rating"] = examples["rating"]
    # åœ¨ input_ids å‰åŠ ä¸Š BOS token
    for key, value in tokenizer(tokenizer.bos_token).items():
        encode_inputs[key] = value + encode_inputs[key]
    
    # åœ¨ input_ids ååŠ ä¸Š EOS token
    for key, value in tokenizer(tokenizer.eos_token).items():
        encode_inputs[key] = encode_inputs[key] + value
        
    return encode_inputs

# ---------------------------------------------
# ç±»å½¢å¼å°è£…çš„è§£é‡Šç”Ÿæˆæ•°æ®é¢„å¤„ç†
# ---------------------------------------------
class Process_Explain_data:
    def __init__(self, tokenizer: Optional[PreTrainedTokenizerBase], max_seq_length: int) -> None:
        self.tokenizer = tokenizer
        self.max_seq_length = max_seq_length

    def __call__(self, examples):
        # ç¼–ç  explanation å­—æ®µä¸º input_ids ç­‰ token
        model_inputs = self.tokenizer(examples["explanation"], 
                                      max_length=self.max_seq_length,
                                      truncation=True)
        # é™„åŠ ç»“æ„åŒ–å­—æ®µï¼ˆç”¨äºç”Ÿæˆæ¨¡å‹åšä¸ªæ€§åŒ–è§£é‡Šï¼‰
        model_inputs["user"] = examples["user"]
        model_inputs["item"] = examples["item"]
        model_inputs["rating"] = examples["rating"]
        
        # add prefix and postfix key: input_ids 
        # æ·»åŠ  BOS token åˆ°å‰ç¼€
        for key, value in self.tokenizer(self.tokenizer.bos_token).items():
            model_inputs[key] = value + model_inputs[key]
        
        # æ·»åŠ  EOS token åˆ°åç¼€
        for key, value in self.tokenizer(self.tokenizer.eos_token).items():
            model_inputs[key] = model_inputs[key] + value

        # until this step, the length of each example input_ids is not equal
        return model_inputs


# ---------------------------------------------
# å¯è§†åŒ–æ½œåœ¨ç©ºé—´ä¸­çš„èšç±»åˆ†å¸ƒï¼Œç”¨ t-SNE é™ç»´åˆ° 2D
# ---------------------------------------------
def plot_latent(vae_clu, data_loader, args, epoch):
    with torch.no_grad():# ç¦ç”¨æ¢¯åº¦ï¼Œæé«˜æ•ˆç‡
        Z = [] # å­˜å‚¨ç¼–ç å™¨è¾“å‡ºçš„ latent å‘é‡
        Y = [] # å­˜å‚¨é¢„æµ‹çš„ç±»åˆ«ç´¢å¼•
        vae_clu.eval() # è®¾ç½®æ¨¡å‹ä¸ºè¯„ä¼°æ¨¡å¼
        
        for batch_index,(user, item, rating, _, _) in enumerate(data_loader):
            user = user.to('cuda')
            item = item.to('cuda')
            z1, _, _ = vae_clu.encoder(user, item)  # è·å–ç”¨æˆ·-ç‰©å“çš„ latent è¡¨å¾
            y = vae_clu.predict(user,item)# è·å–èšç±»ç±»åˆ«ç´¢å¼•
            Y.append(torch.tensor(y))
            Z.append(z1)
        # [batch, latent_dim]
        Z = torch.cat(Z, 0).detach().cpu().numpy() # åˆå¹¶å¹¶è½¬ä¸º numpy
        Y = torch.cat(Y, 0).detach().cpu().numpy()
        index_counts = Counter(Y)
        # æ‰“å°æ¯ä¸ªèšç±»å‡ºç°æ¬¡æ•°
        for index, count in index_counts.items():
            print(f"Cluster {index} appears {count} times.")
 
        print(f'ğŸ¤¡ğŸ¤¡ğŸ¤¡ Ploting Latent Space for {args.dataset}')
        num_samples_per_cluster = 300
        # # æ¯ç±»éšæœºé€‰ 300 ä¸ªæ ·æœ¬è¿›è¡Œå¯è§†åŒ–
        indices = []
        for i in range(args.num_cluster):
            indices_in_cluster = np.where(Y == i)[0]
            selected_indices = np.random.choice(indices_in_cluster, num_samples_per_cluster, replace=True)
            indices.extend(selected_indices)

        selected_Z = Z[indices]
        selected_Y = Y[indices]


        tsne = TSNE(n_components=3, init='pca', random_state=42)  
        Z_tsne = tsne.fit_transform(selected_Z)
        # â€”â€” æŠ•å°„åˆ°å•ä½çƒé¢ï¼Œä¾¿äº 3D çƒé¢ç›´è§‚å±•ç¤º
        norms = np.linalg.norm(Z_tsne, axis=1, keepdims=True) + 1e-12
        Z_sph = Z_tsne / norms                   # (N, 3)

        
        # ========== ç»˜å›¾ ==========
        fig = plt.figure(figsize=(14, 14))
        ax  = fig.add_subplot(111, projection="3d")

        
        cmap_name = 'tab10' if args.num_cluster <= 10 else 'tab20'   # è‡ªåŠ¨é€‰ 10 è‰²æˆ– 20 è‰²
        cmap      = get_cmap(cmap_name)
        unique_clusters = np.unique(selected_Y)
        for i, k in enumerate(unique_clusters):
            idx =  selected_Y == k
            color = cmap(i % cmap.N)        # cmap.N = 10 æˆ– 20
            ax.scatter(Z_sph[idx, 0], Z_sph[idx, 1], Z_sph[idx, 2],
                   s=8, alpha=0.8, color=color, 
                   label=f'Cluster {k}')
        # â€”â€” ç»˜åˆ¶å•ä½çƒï¼ˆwireframeï¼‰
        theta = np.linspace(0, 2 * np.pi, 100)
        phi   = np.linspace(0, np.pi,     100)
        THETA, PHI = np.meshgrid(theta, phi)
        X_sph = np.sin(PHI) * np.cos(THETA)
        Y_sph = np.sin(PHI) * np.sin(THETA)
        Z_sph0= np.cos(PHI)
        ax.plot_wireframe(X_sph, Y_sph, Z_sph0,
                      color="gray", linewidth=0.5, alpha=0.2)

        # â€”â€” è§†è§’ & ç¾åŒ–
        ax.view_init(elev=20, azim=60)   # æ—‹è½¬è§’åº¦å¯è‡ªè¡Œè°ƒæ•´
        ax.set_box_aspect([1, 1, 1])     # ä¿è¯çƒä½“ä¸è¢«æ‹‰ä¼¸
        ax.set_axis_off()                # éšè—åæ ‡è½´
        ax.legend(loc="upper left", fontsize=10, frameon=False)

         # â€”â€” ä¿å­˜

        plt.savefig(os.path.join(args.pretrain_weight_save,args.dataset, args.dataset + '_' + f'latent_vis_cluster_{args.num_cluster}_epoch_{epoch}.png'),dpi=300, bbox_inches="tight")
        plt.show()
        print(f'Plot Latent Space Done for {epoch}')



# ---------------------------------------------
# ç»™å­—å…¸ä¸­æŸä¸ªé”®æ‰©å±•å€¼ï¼ˆå€¼ä¸º listï¼‰
# ---------------------------------------------
def dict_extend(dict, key, value):
    """
        extend the list value of key in dict
    """
    if key in dict and isinstance(value,list):
            dict[key].extend(value)
    else:
        dict[key] = value 
# ---------------------------------------------
# åˆ¤æ–­ç»“æ„ä¸­æ˜¯å¦åŒ…å« Tensorï¼ˆé€’å½’ï¼‰
# ---------------------------------------------
def has_tensor(obj) -> bool:
    """
    Given a possibly complex data structure,
    check if it has any torch.Tensors in it.
    Credit: AllenNLP
    """
    if isinstance(obj, torch.Tensor):
        return True
    elif isinstance(obj, dict):
        return any(has_tensor(value) for value in obj.values())
    elif isinstance(obj, (list, tuple)):
        return any(has_tensor(item) for item in obj)
    else:
        return False

# ---------------------------------------------
# è‡ªå®šä¹‰ Trainerï¼Œé‡å†™é‡‡æ ·å™¨ï¼Œå¼ºåˆ¶é¡ºåºåŠ è½½æ•°æ®ï¼ˆé€‚ç”¨äºæ¨èï¼‰
#æ¨èç³»ç»Ÿå¾€å¾€å…·æœ‰ç”¨æˆ·è¡Œä¸ºæ—¶åºæ€§æˆ–ç”¨æˆ·-ç‰©å“äº¤äº’å¯¹çš„ç»“æ„åŒ–ç‰¹ç‚¹ï¼Œæ‰“ä¹±é¡ºåºä¼šç ´åè¿™ç§æ—¶åºæˆ–ç»“æ„ä¸€è‡´æ€§
# --------------------------------------------
class RecTrainer(Trainer):
    def __init__(self, *args, save_lora = True, **kwargs):
        self.save_lora = save_lora# æ§åˆ¶æ˜¯å¦ä¿å­˜ LoRA å¾®è°ƒç»“æœ
        super().__init__(*args, **kwargs)#ç„¶åè°ƒç”¨çˆ¶ç±»åˆå§‹åŒ– Trainerï¼ˆæ¨¡å‹ã€æ•°æ®é›†ã€è®­ç»ƒå‚æ•°ç­‰ï¼‰ã€‚

    def _get_train_sampler(self) -> Optional[torch.utils.data.Sampler]:
        
        return SequentialSampler(self.train_dataset)## å¼ºåˆ¶ä½¿ç”¨é¡ºåºé‡‡æ ·å™¨ï¼Œé¿å…æ‰“ä¹±é¡ºåº


import torch
from torch.utils.data import DataLoader

# ---------------------------------------------
# å¯¹å®Œæ•´æ•°æ®è¿›è¡Œèšç±»é¢„æµ‹ï¼Œç”Ÿæˆ gate indexï¼ˆé—¨æ§è·¯ç”±ï¼‰åˆ—è¡¨
# å¯¹è¾“å…¥çš„ HuggingFace æ ¼å¼æ•°æ®é›†ä¸­çš„æ‰€æœ‰ç”¨æˆ·-ç‰©å“å¯¹ï¼Œä½¿ç”¨è®­ç»ƒå¥½çš„ VAE + GMM æ¨¡å‹è¿›è¡Œèšç±»é¢„æµ‹ï¼Œå¾—åˆ°æ¯ä¸ªæ ·æœ¬æ‰€å±çš„ç°‡ç¼–å·ï¼ˆå³é—¨æ§ç´¢å¼•ï¼‰
# ---------------------------------------------
def save_gate_index(hf_dataset, vae_clu, batch_size=1000):
    cluster_index_list = []# åˆå§‹åŒ–ä¸€ä¸ªç©ºåˆ—è¡¨ï¼Œç”¨äºä¿å­˜æ¯ä¸ªæ ·æœ¬çš„èšç±»ï¼ˆç°‡ï¼‰ç¼–å·ã€‚
    #ç§»é™¤ä¸å¿…è¦çš„å­—æ®µï¼Œåªä¿ç•™ user å’Œ item ä¸¤åˆ—ï¼Œå› ä¸ºèšç±»é¢„æµ‹åªéœ€è¦ç”¨æˆ·å’Œç‰©å“çš„ IDã€‚
    hf_dataset = hf_dataset.remove_columns(['labels','feature', 'input_ids', 'attention_mask','rating'])
    data_loader = DataLoader(hf_dataset, batch_size=batch_size, shuffle=False)#ä½¿ç”¨ PyTorch çš„ DataLoader å°è£…æ•°æ®ï¼Œæ–¹ä¾¿æŒ‰æ‰¹æ¬¡è¿›è¡Œæ¨ç†ï¼ˆä¸æ‰“ä¹±é¡ºåºï¼‰ã€‚

    print('Processing the gate index...')
    total_batches = len(data_loader)
    processed_batches = 0

    for batch in data_loader:
        users = torch.tensor(batch['user']).to(vae_clu.device)
        items = torch.tensor(batch['item']).to(vae_clu.device)

        indices = vae_clu.predict(users, items)#ä½¿ç”¨è®­ç»ƒå¥½çš„æ¨¡å‹ vae_clu çš„æ–¹æ³• predict_cluster_index()ï¼Œå¯¹å½“å‰ batch çš„æ¯ä¸ªæ ·æœ¬é¢„æµ‹å®ƒå±äºå“ªä¸ªç°‡ï¼ˆcluster indexï¼‰
        cluster_index_list.extend(indices.tolist()) # æŠŠå½“å‰ batch çš„èšç±»ç¼–å·ç»“æœè¿½åŠ è¿›æ€»åˆ—è¡¨ä¸­ã€‚
    
        processed_batches += 1
        if processed_batches % 1000 == 0:
            print(f'process {processed_batches} / {total_batches}')
    
    print(f'Save Gate Index List Length: {len(cluster_index_list)}')
    return cluster_index_list
# ---------------------------------------------
# è‹±æ–‡å¥å­è§„èŒƒåŒ–å¤„ç†ï¼Œæ–¹ä¾¿ tokenizer åˆ†è¯
# ---------------------------------------------
def postprocessing(string):
    '''
    adopted from https://github.com/yoonkim/CNN_sentence/blob/master/process_data.py
    '''
    string = re.sub('\'s', ' \'s', string)
    string = re.sub('\'m', ' \'m', string)
    string = re.sub('\'ve', ' \'ve', string)
    string = re.sub('n\'t', ' n\'t', string)
    string = re.sub('\'re', ' \'re', string)
    string = re.sub('\'d', ' \'d', string)
    string = re.sub('\'ll', ' \'ll', string)
    string = re.sub('\(', ' ( ', string)
    string = re.sub('\)', ' ) ', string)
    string = re.sub(',+', ' , ', string)
    string = re.sub(':+', ' , ', string)
    string = re.sub(';+', ' . ', string)
    string = re.sub('\.+', ' . ', string)
    string = re.sub('!+', ' ! ', string)
    string = re.sub('\?+', ' ? ', string)
    string = re.sub(' +', ' ', string).strip()
    return string