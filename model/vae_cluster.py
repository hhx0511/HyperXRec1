'''
    vae_cluster.py: 
        1ã€ Pretraining VAE to get prior#é¢„è®­ç»ƒ VAEï¼ˆå˜åˆ†è‡ªç¼–ç å™¨ï¼‰ä»¥è·å¾—é«˜è´¨é‡æ½œåœ¨è¡¨ç¤ºå’Œ GMM å…ˆéªŒ
        2ã€design Encoder and Decoder for rating construct# å®šä¹‰ Encoder / Decoder ç»“æ„ï¼Œç”¨äºé‡æ„ç¦»æ•£è¯„åˆ† (1â€“5 stars)
        3ã€design elbo loss#å®šä¹‰åŸºäº GMM çš„ ELBO æŸå¤± (é‡æ„é¡¹ + KL é¡¹)
'''
import os
os.environ['OPENBLAS_NUM_THREADS'] = '8'#  # é™åˆ¶ BLAS çº¿ç¨‹ï¼Œé˜²æ­¢æœåŠ¡å™¨è¿‡è½½
# os.environ['CUDA_LAUNCH_BLOCKING'] = '3'
import sys
import math
import time
import torch
import torch.nn as nn       
import numpy as np
from rich.console import Console
from rich.progress import track
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.manifold import TSNE
import torch.nn.functional as F
from utils.pepler_dataloader import DataLoader_Rs
from sklearn.mixture import GaussianMixture
from torch.utils.data import DataLoader,Dataset
import itertools
from utils.pepler_dataloader import Dataset_Rs_Pytorch
from utils.lr_utils import WarmUpLR
from rich.progress import BarColumn, Progress
from rich.live import Live
from rich.console import Console
from tqdm import tqdm
import datetime

# ----------------------------- é¡¹ç›®å†…éƒ¨ä¾èµ– -----------------------------
from dsvae.utils import init_weights, d_besseli, besseli# æƒé‡åˆå§‹åŒ–å·¥å…·ï¼Œé»˜è®¤ï¼šæ­£æ€åˆ†å¸ƒ + Kaiming å˜ä½“  # ç¬¬ä¸€ç±»ä¿®æ­£è´å¡å°”å‡½æ•° I_v(x) çš„ä¸€é˜¶å¯¼æ•°ï¼Œç”¨äºæœŸæœ›è®¡ç®—# ç¬¬ä¸€ç±»ä¿®æ­£è´å¡å°”å‡½æ•° I_v(x)
from dsvae.config import DEVICE# è¯»å–å…¨å±€è®¾å¤‡é…ç½® ("cuda" / "cpu")
from dsvae.model import VMFMM
from vmfmix.von_mises_fisher import VonMisesFisher, HypersphericalUniform # å¯é‡å‚æ•°åŒ– vMF åˆ†å¸ƒå®ç° # å•ä½çƒé¢ä¸Šçš„å‡åŒ€åˆ†å¸ƒï¼ˆæ­¤å¤„æœªç”¨åˆ°ï¼‰
# vMF èšç±»åº“ï¼ˆä½œè€…è‡ªå®šä¹‰å®ç°ï¼‰
from vmfmix.vmf import VMFMixture
console = Console()



'''
    Encoder: map user-item pair into latent space
    # --------------------------------------------------------------------------------------
    # Encoder: æŠŠ [user, item] åµŒå…¥æ‹¼æ¥åé€è¿›ä¸¤å±‚ TransformerEncoderï¼Œ
    #          è¾“å‡ºæ½œåœ¨åˆ†å¸ƒçš„å‡å€¼ Î¼ å’Œ log ÏƒÂ²ã€‚
    # WHY:
    #   * ç”¨æˆ·/ç‰©å“äº¤äº’æ¨¡å¼å¯èƒ½éçº¿æ€§ï¼ŒTransformer çš„è‡ªæ³¨æ„åŠ›èƒ½å­¦ä¹ é«˜é˜¶å…³ç³»ã€‚
    #   * å°† Î¼, log ÏƒÂ² ç›´æ¥å›å½’ï¼Œå¯åœ¨ KL é¡¹é‡Œä¸ GMM å…ˆéªŒé—­å¼è®¡ç®—ã€‚
    # --------------------------------------------------------------------------------------
'''
class Encoder(nn.Module):
    def __init__(self,n_user,n_item,embedding_size,latent_dim, r=10.0):#r_init=1.0
        super(Encoder,self).__init__()
        self.user_embeddings = nn.Embedding(n_user, embedding_size)#embedding_size768
        self.item_embeddings = nn.Embedding(n_item, embedding_size)

        self.user_embeddings.weight.data.uniform_(-0.2, 0.2)
        self.item_embeddings.weight.data.uniform_(-0.2, 0.2)
        # ä¸¤å±‚ Transformer Encoderï¼šd_model = user+item æ‹¼æ¥åçš„ç»´åº¦
        self.encoder_layer = nn.TransformerEncoderLayer(d_model = embedding_size * 2, nhead = 4, dim_feedforward = 512)#æ¯ä¸ªä½ç½®çš„å‰é¦ˆç¥ç»ç½‘ç»œä¸­éšè—å±‚çš„ç»´åº¦æ˜¯ 512ï¼ˆé»˜è®¤ä½¿ç”¨ä¸¤ä¸ªçº¿æ€§å±‚ï¼‰
        self.transformer_encoder = nn.TransformerEncoder(self.encoder_layer, num_layers = 2)#å®šä¹‰äº†ä¸€ä¸ªåŒ…å«ä¸¤å±‚çš„ TransformerEncoderï¼Œæ¯å±‚éƒ½æ˜¯æ³¨æ„åŠ› + å‰é¦ˆç½‘ç»œçš„ç»“æ„ã€‚
        self.r = r  # <-- å˜ä¸ºåŠ¨æ€å¯è°ƒçš„ r
        # mu and sigma
        ##self.mu_l = nn.Linear(embedding_size * 2, latent_dim)
        ##self.logvar_l = nn.Linear(embedding_size * 2, latent_dim)
        # â–º è¾“å‡º Î¼, Îº åˆ†æ”¯
        self.mu = nn.Linear(embedding_size * 2, latent_dim)#D ç»´å•ä½å‘é‡ï¼Œè¡¨ç¤ºå¹³å‡æ–¹å‘ï¼›
        self.k = nn.Linear(embedding_size* 2, 1)  #â€ƒâ€¢ Îº (â€œkappaâ€ / â€œkâ€) æ˜¯ æ ‡é‡ï¼Œè¡¨ç¤ºè¯¥æ–¹å‘ä¸Šçš„é›†ä¸­åº¦ï¼ˆÎº è¶Šå¤§ï¼Œæ ·æœ¬è¶Šâ€œæŒ¤â€åœ¨ Î¼ é™„è¿‘ï¼‰ã€‚

    def forward(self, user, item):
        user_embedding = self.user_embeddings(user)
        item_embedding = self.item_embeddings(item)
        ui_embedding = torch.cat([user_embedding, item_embedding], dim=1)
        # â‘  é¢„å½’ä¸€åŒ–+dropout å¯æ˜¾è‘—ç¨³ä½æ•°å€¼
        ui_emb   = F.dropout(ui_embedding, p=0.1, training=self.training)
        ui_emb   = F.layer_norm(ui_emb, ui_emb.shape[-1:])
        ui_emb = ui_emb.unsqueeze(0)  #(1, B, 768*2)
        ui_out = self.transformer_encoder(ui_emb) #(1, B, 768*2)
        ui_out = ui_out.squeeze(0)  #  # (B, 768*2)
        #ä»è¿™é‡Œæ”¹
        mu = self.mu(ui_out)
        # ----- 2) é›†ä¸­åº¦ Îº -----
        # softplus ä¿è¯ Îº > 0ï¼›åŠ å¸¸æ•° r åšã€åœ°æ¿ã€é˜²æ­¢æ—©æœŸ Îº å¤ªå¤§ or ä¸º 0
        # We limit kappa to be greater than a certain threshold, because larger kappa will make the cluster more compact.
        raw_k_in = torch.clamp(self.k(ui_out), min=-15.0, max=15.0)   # é™å¹… â†’ é¿å… exp(Â±âˆ)
        #print(" raw_k_in min/max:", raw_k_in.min(), raw_k_in.max())
        k = F.softplus(raw_k_in)+self.r  # <-- å˜ä¸ºåŠ¨æ€å¯è°ƒçš„ r
        k = torch.clamp(k, max=50.0)
        #print(" k min/max:", k.min(), k.max())
        if torch.isnan(k).any(): raise ValueError("NaN in k")
        # ----- 3) é‡å‚æ•°åŒ–é‡‡æ · z ~ vMF(Î¼, Îº) -----
        # VonMisesFisher ç±»å®ç°äº† `rsample()`ï¼Œå› æ­¤å¯åå‘ä¼ æ’­
        mu = mu / (mu.norm(dim=1, keepdim=True) + 1e-8) # å•ä½åŒ–ï¼Œè½åœ¨ S^{D-1}
        if torch.isnan(mu).any(): raise ValueError("NaN in mu")
        #print("mu", mu)
        z = VonMisesFisher(mu, k).rsample()# (B, D)#å¦‚æœä½ æ²¡æœ‰ä¼  shapeï¼Œå®ƒå°±æ˜¯ torch.Size()ï¼Œå³ []è¿™è¡¨ç¤ºå¯¹ æ¯ä¸ªåˆ†å¸ƒé‡‡ 1 ä¸ªæ ·æœ¬
        return z, mu, k
'''
    Decoder: decoder the rating
    # --------------------------------------------------------------------------------------
    # Decoder: æŠŠæ½œåœ¨å‘é‡ z æ˜ å°„å› 5 çº§è¯„åˆ† logitsã€‚
    # WHY: è¯„åˆ†æ˜¯ç¦»æ•£ 1-5ï¼Œéœ€è¦ 5-way softmaxï¼›ç›´æ¥ç”¨å¤šå±‚æ„ŸçŸ¥æœºè¶³å¤Ÿã€‚
    # --------------------------------------------------------------------------------------
'''
class Decoder(nn.Module):
    def __init__(self,latent_dim):
        super(Decoder, self).__init__()
        self.fc1 = nn.Sequential(
            nn.Linear(latent_dim, 512),
            nn.GELU(),
            nn.Linear(512, 256),
            nn.GELU(),

            nn.Linear(256, 5))# 5 ç±» logits
    def forward(self,z):
        z = self.fc1(z)# (B, 5)
        return z

# --------------------------------------------------------------------------------------
# Vae_Cluster_Es: æ•´ä½“ VAE + GMM æ¨¡å‹
#     * å…ˆç”¨ VAE é¢„è®­ç»ƒ â€”â€”> å¾—åˆ°æ½œåœ¨è¡¨ç¤º Z
#     * ç”¨é«˜æ–¯æ··åˆåˆå§‹åŒ– GMM å…ˆéªŒ (Ï†, Î¼_c, ÏƒÂ²_c)
#     * è®­ç»ƒæ—¶åœ¨ ELBO ä¸­æ˜¾å¼è®¡ç®— KL(q(z|x) || p(z))ï¼Œå…¶ä¸­ p(z) ä¸º GMM
# WHY: â€œå…ˆè‡ªç¼–ç å†èšç±»â€ çš„å˜åˆ†æ·±åº¦èšç±»æ€æƒ³ (å¦‚ VaDE)ã€‚
# --------------------------------------------------------------------------------------
class Vae_Cluster_Es(nn.Module):
    '''
        args: 
            n_user: number of user
            n_item: number of item
            args  : parser parameter
    '''
    def __init__(self, n_user, n_item, args):
        super(Vae_Cluster_Es, self).__init__()

        self.encoder      = Encoder(n_user = n_user, n_item = n_item, embedding_size=args.embedding_size, latent_dim=args.latent_dim)
        self.decoder      = Decoder(latent_dim = args.latent_dim)
        self.device       = 'cuda' if torch.cuda.is_available() else 'cpu'

        # ---- å‚æ•°åˆå§‹åŒ– ----
        # Ï€_cï¼šåˆå§‹åŒ–ä¸ºå‡åŒ€åˆ†å¸ƒ (1/K)
        # Î¼_cï¼šéšæœºå‘é‡ â†’ å•ä½åŒ–ï¼Œæ¯ç°‡å‡å€¼ 
        # Îº_cï¼šåœ¨åŒºé—´ [1, 5] å‡åŒ€åˆå§‹åŒ–ï¼ˆæ•°å€¼è¿‡å¤§è®­ç»ƒéš¾ï¼Œè¿‡å°åˆ†å¸ƒè¿‡äºæ‰å¹³ï¼‰
        mu = torch.FloatTensor(args.num_cluster, args.latent_dim).normal_(0, 0.02)
        self.pi_ = nn.Parameter(torch.FloatTensor(args.num_cluster, ).fill_(1) / args.num_cluster, requires_grad=True)
        self.mu_c = nn.Parameter(mu / mu.norm(dim=-1, keepdim=True), requires_grad=True)
        self.k_c = nn.Parameter(torch.FloatTensor(args.num_cluster, ).uniform_(1, 5), requires_grad=True)
        # GMM å‚æ•°ï¼šå…ˆéšæœº/é›¶åˆå§‹åŒ–ï¼Œé¢„è®­ç»ƒåä¼šè¢« GMM æ‹Ÿåˆå€¼è¦†ç›–
        ##self.phi          = nn.Parameter(torch.FloatTensor(args.num_cluster,).fill_(1) / args.num_cluster, requires_grad=True)# æ··åˆæƒé‡
        ##self.mu_c         = nn.Parameter(torch.FloatTensor(args.num_cluster, args.latent_dim).fill_(0), requires_grad=True)# æ¯ç°‡å‡å€¼
        ##self.log_sigma2_c = nn.Parameter(torch.FloatTensor(args.num_cluster, args.latent_dim).fill_(0), requires_grad=True)# æ¯ç°‡å¯¹è§’ log ÏƒÂ²
        self.args         = args


    def now_time(self):
        return '[' + datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S.%f') + ']: '
    def plot_loss_curve(self, losses, title='Pretraining Loss Curve', save_path='loss_curve.png', dpi=200):
        """ä¿å­˜è®­ç»ƒæ›²çº¿"""
        plt.figure(figsize=(10, 5))

        sns.lineplot(x = range(len(losses)), y = losses,marker='*',markerfacecolor='#F0988C', markersize=16, markevery=10)
        plt.gca().lines[0].set_color('#C76DA2')
        plt.gca().lines[0].set_linestyle('-')
        plt.gca().lines[0].set_linewidth(2.5)

        plt.xlabel('Epoch')
        plt.ylabel('Loss')
        plt.title(title)
        plt.grid(True)
        plt.savefig(save_path, dpi=dpi)
        plt.close()
    def plot_accuracy_curve(self, accuracies, title='Accuracy per Epoch', save_path='accuracy_curve.png', dpi=200):
        plt.figure(figsize=(10, 5))
        sns.lineplot(x=range(len(accuracies)), y=accuracies, marker='o',markerfacecolor='#F0988C', markersize=16, markevery=10)
        plt.xlabel('Epoch')
        plt.ylabel('Clustering Accuracy (%)')
        plt.title(title)
        plt.grid(True)
        plt.savefig(save_path, dpi=dpi)
        plt.close()


    def cluster_acc(self,Y_pred, Y):
        import scipy.io as scio
        from scipy.optimize import linear_sum_assignment  # æ›¿ä»£æ—§ linear_assignment
        assert Y_pred.size == Y.size
        D = max(Y_pred.max(), Y.max())+1
        w = np.zeros((D,D), dtype=np.int64)
        for i in range(Y_pred.size):
            w[Y_pred[i], Y[i]] += 1
        ind = linear_sum_assignment(w.max() - w)
        return sum([w[i,j] for i,j in zip(*ind)])*1.0/Y_pred.size, w

    # å…ˆç”¨é¢„è®­ç»ƒæŠŠlatent spaceè®­ç»ƒå¥½ å¾—åˆ°å…ˆéªŒåˆ†å¸ƒ
    # ==================================================================================
    # Phase-1: ä»…ä¼˜åŒ– Encoder+Decoder â€”â€” é‡æ„è¯„åˆ†ï¼Œç›®çš„æ˜¯å­¦å‡ºç¨³å®šæ½œåœ¨ç©ºé—´ä½œä¸º GMM å…ˆéªŒ
    # ==================================================================================
    def pretrain(self, corpus, pretrain_epoch):
        assert self.args.pretrain_weight_save is not None#"å¿…é¡»æŒ‡å®š --pretrain_weight_save"
        print(f'Start Pretraining !!!!!')
        # å¦‚æœæ²¡æœ‰é¢„è®­ç»ƒæƒé‡æ–‡ä»¶ï¼Œæ‰å¼€å§‹è®­ç»ƒæµç¨‹
        if not os.path.exists(os.path.join(os.path.join(self.args.pretrain_weight_save,self.args.dataset, self.args.dataset + f'_cluster_{self.args.num_cluster}_'  +'pretrain_weight.pth'))):
            warm = 1  # é¢„çƒ­ epoch æ•°
            Loss = nn.CrossEntropyLoss()        
            optimizer = torch.optim.Adam(itertools.chain(self.encoder.parameters(),self.decoder.parameters()),lr = 0.00015)# 0.00015
            train_scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones = [60, 120], gamma = 0.9) #learning rate decay# å­¦ä¹ ç‡è°ƒåº¦å™¨ï¼šåœ¨ç¬¬ 60 å’Œ 120 epoch æ—¶è¡°å‡,æ¯æ¬¡è§¦å‘æ—¶ï¼Œå°†å­¦ä¹ ç‡ä¹˜ä»¥ 0.9ï¼ˆå³ä¸‹é™ 10%ï¼‰
            # batch_size influences the Training Time in V100 32GB. ex: 81920 vs 1024, meanwhile, which influences the result of clustering
            # btw: using Adam optimizer, remember to set small batch size like 256,512
            # è®­ç»ƒæ•°æ®åŠ è½½å™¨
            data_loader = DataLoader(Dataset_Rs_Pytorch(corpus.train),batch_size = 2048,shuffle = True)#ï¼Œåªä½¿ç”¨äº† corpus.train æ¥è¿›è¡Œ VAE çš„é¢„è®­ç»ƒã€‚user, item, rating , text, feature
            iter_per_epoch = len(data_loader)
            warmup_scheduler = WarmUpLR(optimizer, iter_per_epoch * warm)# WarmUpLRï¼šé¢„çƒ­è°ƒåº¦å™¨ï¼Œç”¨äºå‰ warm ä¸ª epoch å†…é€æ­¥æå‡å­¦ä¹ ç‡ï¼Œé˜²æ­¢è®­ç»ƒåˆæœŸä¸ç¨³å®š
            print("================================================ Pretraining Start ================================================")
            epoch_bar = tqdm(range(pretrain_epoch))
            start = time.time()
            losses = []
            
            best_val_loss = float('inf') # åˆå§‹åŒ–æœ€ä½³ loss
            endure_count = 0
            endure_count_break = 15#å¦‚æœè¿ç»­ 15 æ¬¡éƒ½æ²¡æœ‰æå‡ï¼Œå°±åœæ­¢è®­ç»ƒ
            for epoch in epoch_bar:
                total_sample = 0. 
                losses_epoch = 0.
                epoch = epoch + 1
                if epoch > warm:
                    train_scheduler.step(epoch)#åœ¨å‰ warm=1 ä¸ª epoch ä¸­ï¼Œæˆ‘ä»¬ä½¿ç”¨çš„æ˜¯ WarmUpLRï¼Œå³å­¦ä¹ ç‡çº¿æ€§å¢é•¿è°ƒåº¦å™¨,åœ¨ç¬¬ warm+1 ä¸ª epoch ä¹‹åï¼Œæ‰å¯ç”¨ MultiStepLRï¼Œæ ¹æ®è®­ç»ƒè½®æ•°è°ƒæ•´å­¦ä¹ ç‡ï¼›
                for batch_index,(user, item, rating, _, _) in enumerate(data_loader):
                    user = user.to(self.device)
                    item = item.to(self.device)
                    rating = rating - 1 #å°†åŸå§‹è¯„åˆ†è½¬æ¢æˆä» 0 å¼€å§‹çš„ç±»åˆ«ç´¢å¼•,å› ä¸º nn.CrossEntropyLoss() è¦æ±‚ç›®æ ‡æ ‡ç­¾ï¼ˆtargetï¼‰æ˜¯ ä» 0 å¼€å§‹çš„æ•´æ•°åˆ†ç±»æ ‡ç­¾ï¼Œæ‰€ä»¥éœ€è¦å¯¹åŸå§‹çš„è¯„åˆ† rating æ‰§è¡Œ -1 æ“ä½œã€‚
                    rating = rating.to(self.device)
                    optimizer.zero_grad()
                    #è¿™é‡Œæ”¹äº†
                    z, _, _  = self.encoder(user,item)
                    pre_rating = self.decoder(z)
                    loss = Loss(pre_rating, rating)
                    losses_epoch += loss
                    loss.backward()
                    optimizer.step()
                    if batch_index % 100 == 0:#è¡¨ç¤ºæ¯è®­ç»ƒ 100 ä¸ª batch å°±è®°å½•ä¸€æ¬¡å½“å‰çš„ loss å€¼ï¼ŒåŠ å…¥ loss æ›²çº¿ä¸­ï¼Œæ–¹ä¾¿åç»­å¯è§†åŒ–è®­ç»ƒæ”¶æ•›è¿‡ç¨‹ã€‚
                        losses.append(loss.item())
                    console.print(':thumbs_up: :pile_of_poo: Time:{time} Training Epoch: {epoch}/{all_epoch} [{trained_samples}/{total_samples}]\tLoss: {:0.4f}\tLR: {:0.6f}'.format(
                            loss.item(),
                            optimizer.param_groups[0]['lr'],
                            time = self.now_time(),
                            epoch=epoch,
                            all_epoch=pretrain_epoch,
                            trained_samples=batch_index * data_loader.batch_size + len(user),#è®­ç»ƒè¿›åº¦ç»Ÿè®¡ï¼šbatch_index * batch_size æ˜¯ä¹‹å‰å¤„ç†è¿‡çš„æ ·æœ¬æ•°ï¼Œlen(user) æ˜¯å½“å‰ batch çš„å¤§å°# æ‰€ä»¥æ€»çš„å·²å¤„ç†æ ·æœ¬æ•°ä¸º (å‰é¢ batch çš„æ ·æœ¬æ•° + å½“å‰ batch æ ·æœ¬æ•°)
                            total_samples=len(data_loader.dataset)
                    ), style="bold red")
                    total_sample += data_loader.batch_size
            
                if epoch <= warm:
                    warmup_scheduler.step()
                losses_epoch = losses_epoch / len(data_loader)
                if losses_epoch < best_val_loss:   #å¦‚æœæœ¬è½®è®­ç»ƒï¼ˆä¸€ä¸ª epochï¼‰çš„å¹³å‡ lossï¼ˆlosses_epochï¼‰å°äºå½“å‰è®°å½•çš„æœ€ä½³ lossï¼ˆbest_val_lossï¼‰ï¼Œè¯´æ˜æ¨¡å‹æ€§èƒ½æå‡äº†ã€‚   
                    best_val_loss = losses_epoch
                    torch.save(self.state_dict(), os.path.join(self.args.pretrain_weight_save,self.args.dataset,self.args.dataset + f'_cluster_{self.args.num_cluster}_' + 'best_' +'pretrain_weight.pth'))
                    print(f'Saving Best Pretraining Model for loss {best_val_loss}')
                else:
                    endure_count += 1#å¦‚æœå½“å‰ epoch çš„ loss æ²¡æœ‰å˜å¥½ï¼Œè¯´æ˜è®­ç»ƒæ€§èƒ½åœæ»ï¼Œé‚£å°±ç´¯åŠ è€å¿ƒè®¡æ•°å™¨ endure_countã€‚
                    console.print(self.now_time() + 'We are going to early stop..., Which is Harder...')
                    if endure_count == endure_count_break:
                        break
             
            finish = time.time() 
            print('epoch {} training time consumed: {:.2f}s'.format(epoch, finish - start))#æ‰“å°æ•´ä¸ªè®­ç»ƒæ‰€è€—è´¹çš„æ—¶é—´ï¼Œå¸®åŠ©è¯„ä¼°è®­ç»ƒæ•ˆç‡ã€‚
            #æŠŠ encoder ä¸­ mu_lï¼ˆå‡å€¼åˆ†æ”¯ï¼‰çš„å‚æ•° å®Œæ•´æ‹·è´ åˆ° logvar_lï¼ˆå¯¹æ•°æ–¹å·®åˆ†æ”¯ï¼‰,æ¨¡å‹åˆå§‹åŒ–æ—¶ä»¤ç¼–ç å™¨çš„å‡å€¼å’Œæ–¹å·®ç›¸åŒï¼ˆå†·å¯åŠ¨ï¼Œé¿å…å·®å¼‚å¤ªå¤§å¯¼è‡´è®­ç»ƒä¸ç¨³å®šï¼‰
            #é€šå¸¸ç”¨äºé¢„è®­ç»ƒç»“æŸåï¼Œåˆå§‹ GMM èšç±»å‰ï¼Œç¡®ä¿éšå˜é‡çš„å‡å€¼å’Œæ–¹å·®ç¼–ç å…·æœ‰ä¸€è‡´æ€§ï¼Œä¾¿äºä¹‹å GMM çš„èšç±»ç¨³å®šæ”¶æ•›ã€‚
            #self.encoder.logvar_l.load_state_dict(self.encoder.mu_l.state_dict())
            self.plot_loss_curve(losses, title='Pretrain Loss Curve',save_path=os.path.join(self.args.pretrain_weight_save,self.args.dataset,self.args.dataset + '_' + 'pretrain_loss.png'))
            
            Z = []
            with torch.no_grad():
                for user, item, rating, _, _ in data_loader:
                    user = user.to(self.device)
                    item = item.to(self.device)
                    #è¿™é‡Œæ”¹äº†
                    z, mu, k   = self.encoder(user,item)
                    #assert F.mse_loss(z1, z2) == 0
                    Z.append(z)#ä¾‹Z = [Tensor[2048, 10], Tensor[2048, 10], ..., Tensor[1024, 10]]
            # Z shape : batch,latent_dim
            Z = torch.cat(Z, 0).detach().cpu().numpy()#)#æ­¤æ—¶ encoder è¾“å‡ºçš„æ˜¯ç¡®å®šæ€§çš„ zï¼ˆå› ä¸ºæ²¡æœ‰é‡‡æ ·ï¼‰ï¼Œtorch.cat(Z, 0)	æ²¿ç¬¬ 0 ç»´ï¼ˆæ ·æœ¬ç»´ï¼‰å°†å¤šä¸ª batch æ‹¼æ¥æˆä¸€ä¸ªå¤§å¼ é‡
          
            ##gmm = GaussianMixture(n_components = self.args.num_cluster, covariance_type='diag')#ç”¨ GMM åœ¨éšç©ºé—´ z ä¸Šèšç±»
            ##pre = gmm.fit_predict(Z)#è¿”å›çš„æ˜¯ä¸€ä¸ªæ•´æ•°å‘é‡ï¼Œå…¶ä¸­æ¯ä¸ªå€¼æ˜¯è¯¥æ ·æœ¬æ‰€å±çš„èšç±»æ ‡ç­¾ï¼ˆcluster IDï¼‰ï¼Œå–å€¼èŒƒå›´æ˜¯ 0 ~ num_clusters - 1ã€‚
            _vmfmm = VMFMixture(n_cluster=self.args.num_cluster, max_iter=100)#æ¥æºäº vmfmix.vmf.VMFMixtureï¼Œæ˜¯ç”¨æ¥åšåˆå§‹èšç±»çš„å·¥å…·ã€‚
            _vmfmm.fit(Z)
            pre = _vmfmm.predict(Z)
            

            #  å¯è§†åŒ–é€‰å–
            num_samples_per_cluster = 500
            indices = []
            for i in range(self.args.num_cluster):
                indices_in_cluster = np.where(pre == i)[0]#æ‰¾å‡ºé¢„æµ‹ä¸ºç¬¬ i ç±»ç°‡çš„æ ·æœ¬ç´¢å¼•ã€‚
                selected_indices = np.random.choice(indices_in_cluster, num_samples_per_cluster, replace=False)#ä»è¿™äº›ç´¢å¼•ä¸­**éšæœºé€‰å‡ºä¸€å®šæ•°é‡ï¼ˆå¦‚ 500ï¼‰**è¿›è¡Œå¯è§†åŒ–ï¼Œreplace=False è¡¨ç¤ºä¸é‡å¤æŠ½æ ·ã€‚
                indices.extend(selected_indices)#indices.extend(...): å°†æ¯ä¸€ç°‡ä¸­é€‰ä¸­çš„æ ·æœ¬ç´¢å¼•æ·»åŠ è¿› indices åˆ—è¡¨ï¼Œæœ€ç»ˆç”¨äºæ‹¼æ¥ä¸€ç»„ç”¨äº t-SNE å¯è§†åŒ–çš„å­é›†ã€‚

            selected_Z = Z[indices]


            tsne = TSNE(n_components=3, init='pca', random_state=42)  
            Z_tsne = tsne.fit_transform(selected_Z)
            # â€”â€” æŠ•å°„åˆ°å•ä½çƒé¢ï¼Œä¾¿äº 3D çƒé¢ç›´è§‚å±•ç¤º
            norms = np.linalg.norm(Z_tsne, axis=1, keepdims=True) + 1e-12
            Z_sph = Z_tsne / norms                   # (N, 3)
            
            selected_pre = pre[indices]#æ¯ä¸ªå¯è§†åŒ–ç‚¹æ‰€å±çš„èšç±»ç¼–å·ã€‚
            
            fig = plt.figure(figsize=(14, 14))
            ax  = fig.add_subplot(111, projection="3d")
            sc = ax.scatter(Z_sph[:, 0], Z_sph[:, 1], Z_sph[:, 2],c=selected_pre, cmap='viridis',  # ä½¿ç”¨èšç±»æ ‡ç­¾è¿›è¡Œç€è‰²
                     s=8, alpha=0.8, edgecolors='k', linewidths=0.2)

            # â€”â€” ç»˜åˆ¶å•ä½çƒï¼ˆwireframeï¼‰
            theta = np.linspace(0, 2 * np.pi, 100)
            phi   = np.linspace(0, np.pi,     100)
            THETA, PHI = np.meshgrid(theta, phi)
            X_sph = np.sin(PHI) * np.cos(THETA)
            Y_sph = np.sin(PHI) * np.sin(THETA)
            Z_sph0= np.cos(PHI)
            ax.plot_wireframe(X_sph, Y_sph, Z_sph0,
                      color="gray", linewidth=0.5, alpha=0.2)

            # â€”â€” è§†è§’ & ç¾åŒ–
            ax.view_init(elev=20, azim=60)   # æ—‹è½¬è§’åº¦å¯è‡ªè¡Œè°ƒæ•´
            ax.set_box_aspect([1, 1, 1])     # ä¿è¯çƒä½“ä¸è¢«æ‹‰ä¼¸
            ax.set_axis_off()                # éšè—åæ ‡è½´
            #fig.colorbar(sc, ax=ax, shrink=0.6)  # æ·»åŠ é¢œè‰²æ¡

             
            plt.title(f'Vis of Pretrain Latent Space for {self.args.dataset}') 
            plt.savefig(os.path.join(self.args.pretrain_weight_save,self.args.dataset, self.args.dataset + '_' + f'pretrain_latent_{self.args.num_cluster}.png'),dpi=300)

            print('VMFMixture Model Fit Done......')
            #è¿™é‡Œæ”¹äº†
            ##self.phi.data = torch.from_numpy(gmm.weights_).cuda().float()
            ##self.mu_c.data = torch.from_numpy(gmm.means_).cuda().float()
            
            self.pi_.data = torch.from_numpy(_vmfmm.pi).cuda().float()
            self.mu_c.data = torch.from_numpy(_vmfmm.xi).cuda().float()
            self.k_c.data = torch.from_numpy(_vmfmm.k).cuda().float()
            # æ³¨æ„è¿™é‡Œå–äº†log
            #self.log_sigma2_c.data = torch.log(torch.from_numpy(gmm.covariances_).cuda().float())
            
            if not self.args.pretrain_weight_save:
                os.mkdirs(self.args.pretrain_weight_save)

            torch.save(self.state_dict(), os.path.join(self.args.pretrain_weight_save,self.args.dataset,self.args.dataset + f'_cluster_{self.args.num_cluster}_'  +'pretrain_weight.pth'))
            self.load_state_dict(torch.load(os.path.join(self.args.pretrain_weight_save,self.args.dataset,self.args.dataset + f'_cluster_{self.args.num_cluster}_'  +'pretrain_weight.pth')))
            print('loaded best pretrain weight')
        else:
            self.load_state_dict(torch.load(os.path.join(self.args.pretrain_weight_save,self.args.dataset, self.args.dataset + f'_cluster_{self.args.num_cluster}_'  +'pretrain_weight.pth')))
            print('already loaded')
    
    
    # ------------------------------------------------------------------
    # æ¨æ–­ (Eâ€‘step)ï¼šç»™å®š z è®¡ç®—è´£ä»»æ¦‚ç‡ Î³_icï¼Œå†å– argmax å¾—ç¦»æ•£æ ‡ç­¾
    # ----------------------------------------------------------------
    def predict(self,user, item):
        """ç»™å®šéšå‘é‡ zï¼Œè¾“å‡ºç¡¬èšç±»æ ‡ç­¾ (numpy array)ã€‚"""
        z, _, _  = self.encoder(user, item)
        pi = self.pi_
        mu_c = self.mu_c
        k_c = self.k_c
        yita_c = torch.exp(torch.log(pi.unsqueeze(0)) + self.vmfmm_pdfs_log(z, mu_c, k_c))

        yita = yita_c.detach().cpu().numpy()
        return np.argmax(yita, axis=1)
    # ------------------------------------------------------------------
    # é‡‡æ ·ï¼šæŒ‡å®šæŸç°‡ (ç´¢å¼• k) çš„æ¡ä»¶ä¸‹ç”Ÿæˆæ–¹å‘å‘é‡ 
    # ------------------------------------------------------------------
    def sample_by_k(self, k, num=10):
        """ä»ç¬¬ k ä¸ªç»„ä»¶ vMF(Î¼_k, Îº_k) é‡‡æ · num ä¸ª zã€‚"""
        mu = self.mu_c[k:k+1]
        k = self.k_c[k].view((1, 1))
        z = None
        for i in range(num):
            _z = VonMisesFisher(mu, k).rsample()
            if z is None:
                z = _z
            else:
                z = torch.cat((z, _z))
        return z
    # ------------------------------------------------------------------
    # è¾…åŠ©å‡½æ•°ï¼šæ‰¹é‡è®¡ç®— log p(z | c=k)å¯¹ zï¼Œè®¡ç®—å®ƒåœ¨æ¯ä¸€ä¸ªvmfåˆ†å¸ƒ ğ‘ğ‘˜ä¸Šçš„å¯¹æ•°æ¦‚ç‡å¯†åº¦ logğ‘(ğ‘§ğ‘–âˆ£ğ‘ğ‘˜)
    # ------------------------------------------------------------------
    def vmfmm_pdfs_log(self, x, mu_c, k_c):

        VMF = []
        for c in range(self.args.num_cluster):
            VMF.append(self.vmfmm_pdf_log(x, mu_c[c:c + 1, :], k_c[c]).view(-1, 1))
        return torch.cat(VMF, 1)
    
    """å•ç»„ä»¶ vMF å¯¹æ•°å¯†åº¦ã€‚
        å…¬å¼ï¼š
            log f(z) = (D/2 - 1) log Îº - D/2 log Ï€ - log I_{D/2-1}(Îº) + Îº Î¼^T z
    """
    @staticmethod
    def vmfmm_pdf_log(x, mu, k):
        D = x.size(1)
        log_pdf = (D / 2 - 1) * torch.log(k) - D / 2 * math.log(math.pi) - torch.log(besseli(D / 2 - 1, k)) \
                  + x.mm(torch.transpose(mu, 1, 0) * k)#	torch.transpose(mu, 1, 0)(D, 1)	æŠŠå‡å€¼æ–¹å‘ Î¼ ä» (1, D) è½¬æˆåˆ—å‘é‡ï¼Œä¾¿äºåç»­çŸ©é˜µä¹˜ï¼Œx.mm(...)ï¼šæ‰¹é‡è®¡ç®—æ¯ä¸ªæ ·æœ¬ä¸ Îº Î¼ çš„ç‚¹ç§¯ã€‚
        return log_pdf
    # ------------------------------------------------------------------
    # å˜åˆ†ä¸‹ç•Œ (ELBO) ä¸­å…³äºèšç±»å…ˆéªŒçš„æœŸæœ›é¡¹ (è´Ÿå·ååšæœ€å°åŒ–)ã€‚
    # ------------------------------------------------------------------
    def vmfmm_Loss(self, user, item, rating, scale_factor_kl):
        """
        å‚æ•°
        ----------
        z    : (B, D)   é‡‡æ ·éšå˜é‡
        z_mu : (B, D)   Encoder è¾“å‡ºæ–¹å‘å‘é‡
        z_k  : (B, 1)   Encoder è¾“å‡ºé›†ä¸­åº¦

        è¿”å›
        ------
        Loss : torch.Tensor æ ‡é‡ï¼›å€¼è¶Šå°è¶Šå¥½ (å·²å–è´ŸæœŸæœ›)
        """
        L                  = 1                            # é‡‡æ ·æ¬¡æ•°ï¼›è®¾ 1 å³å¯ï¼Œçœæ˜¾å­˜çœæ—¶é—´
        z, z_mu, z_k = self.encoder(user, item)     # q(z|x) å‡å€¼ & log ÏƒÂ²
        loss_func          = nn.CrossEntropyLoss()        # ç¦»æ•£ 5-çº§è¯„åˆ† â†’ äº¤å‰ç†µ
        # start sampling   
        # ------------------------- (1) é‡æ„é¡¹ ----------------------------------
        elbo_loss          = 0
        for l in range(L): 
            z = z.to(self.device)
            pre_rating = self.decoder(z)  # (B,5)
            elbo_loss += loss_func(pre_rating, rating)


        Loss           = elbo_loss /  L * self.args.embedding_size * 2#elbo_loss / Lå¦‚æœä½ æ¯æ¬¡é‡‡æ ·å¤šä¸ª zï¼ˆL>1ï¼‰ï¼Œé‚£è¦å¯¹å®ƒä»¬çš„æŸå¤±æ±‚å¹³å‡ã€‚,* self.args.embedding_size * 2è¿™æ˜¯ä¸ºäº†è®© reconstruction loss çš„é‡çº§å’Œ KL é¡¹çš„é‡çº§ç›¸å¯¹ä¸€è‡´ã€‚
        det = 1e-10 # é˜²æ­¢ log(0)
        pi = self.pi_         # (K,)
        mu_c = self.mu_c      # (K, D)
        k_c = self.k_c        # (K,)

        D = self.args.latent_dim#self.n_features
        # ---------- 1) è´£ä»»æ¦‚ç‡ Î³_ic = q(c|z) ----------
        yita_c = torch.exp(torch.log(pi.unsqueeze(0)) + self.vmfmm_pdfs_log(z, mu_c, k_c)) + det
        yita_c = yita_c / (yita_c.sum(1).view(-1, 1))  # batch_size*Clusters # å½’ä¸€åŒ–åˆ°æ¦‚ç‡

        # ---------- 2) E_q[Îº_c Î¼_c^T z] ----------
        # dI_v/dÎº / I_v(Îº) â‰ˆ E[Î¼^T z]ï¼Œå‚è§ vMF çš„æœŸæœ›æ€§è´¨
        # batch * n_cluster
        e_k_mu_z = (d_besseli(D / 2 - 1, z_k) * z_mu).mm((k_c.unsqueeze(1) * mu_c).transpose(1, 0)) # (B, K)
        
        # ---------- 3) E_q[Îº_z Î¼_z^T z] (selfâ€‘term) --------
        # batch * 1
        e_k_mu_z_new = torch.sum((d_besseli(D / 2 - 1, z_k) * z_mu) * (z_k * z_mu), 1, keepdim=True)

        # e_log_z_x
        kl = torch.mean((D * ((D / 2 - 1) * torch.log(z_k) - D / 2 * math.log(math.pi) - torch.log(besseli(D / 2 - 1, z_k)) + e_k_mu_z_new)))

        # e_log_z_c
        kl -= torch.mean(torch.sum(yita_c * (
                D * ((D / 2 - 1) * torch.log(k_c) - D / 2 * math.log(math.pi) - torch.log(besseli(D / 2 - 1, k_c)) + e_k_mu_z)), 1))

        kl -= torch.mean(torch.sum(yita_c * torch.log(pi.unsqueeze(0) / yita_c), 1))
        kl = kl * scale_factor_kl
        Loss += kl
        return Loss * 0.1
    def vmfmm_accuracy(self, user, item, rating):
        """è®¡ç®—èšç±»å‡†ç¡®ç‡ï¼ˆè‡ªåŠ¨åˆ‡æ–­æ¢¯åº¦ï¼Œä¸å‚ä¸åå‘ä¼ æ’­ï¼‰"""
        z, _, _ = self.encoder(user, item)
        tru=rating-1
        z = z.detach().cpu().numpy()         # åˆ‡æ–­æ¢¯åº¦ + è½¬CPU + è½¬NumPy
        tru = tru.detach().cpu().numpy()
        _vmfmm = VMFMixture(n_cluster=self.args.num_cluster, max_iter=100)#æ¥æºäº vmfmix.vmf.VMFMixtureï¼Œæ˜¯ç”¨æ¥åšåˆå§‹èšç±»çš„å·¥å…·ã€‚
        _vmfmm.fit(z)                         # å…ˆè®­ç»ƒèšç±»æ¨¡å‹
        pre = _vmfmm.predict(z)              # ç„¶åæ‰èƒ½é¢„æµ‹
        accuracy=self.cluster_acc(pre,tru)[0]*100
        return   accuracy

    # =============================================================================
    #  Vae_Cluster_Es.Elbo_loss
    #  ---------------------------------
    #  è¾“å…¥:
    #     user, item  : LongTensorï¼Œæ‰¹é‡ç”¨æˆ· / ç‰©å“ç´¢å¼•
    #     rating      : LongTensorï¼ŒçœŸå®è¯„åˆ† (0-4ï¼Œå¯¹é½ CrossEntropyLoss)
    #     scale_factor_kl : floatï¼ŒÎ²-VAE çš„ KL é€€ç«ç³»æ•°
    #
    #  è¾“å‡º:
    #     æ ‡é‡ Loss  â€”â€” äº¤å‰ç†µé‡æ„é¡¹ + Î²Â·KL(GMMâ€–Posterior)
    #
    #  WHY è®¾è®¡è¦ç‚¹
    #  -------------
    #  1) å…ˆé‡‡æ · z é‡æ„è¯„åˆ†ï¼Œå¾—åˆ°   L_rec             ï¼ˆé‡æ„é¡¹ï¼‰
    #  2) å†åˆ©ç”¨ GMM å…ˆéªŒé—­å¼å…¬å¼  L_KL              ï¼ˆæ­£åˆ™é¡¹ï¼‰
    #  3) Î²-é€€ç« (scale_factor_kl) é€æ¸â†‘ï¼Œé˜²æ­¢ posterior collapse
    #  4) æœ€åæ•´ä½“ä¹˜ 0.1 ä¸ä½œè€…ç»éªŒä¿æŒé‡çº§ï¼Œåˆ©äº LR è°ƒå‚
    # =============================================================================
    