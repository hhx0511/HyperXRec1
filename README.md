# HyperXRec: Cluster-Aware Explainable Recommendation via MoE-LLM

**HyperXRec** is a lightweight and modular recommendation framework that combines hyperspherical latent-space clustering with a LoRA-tuned Mixture-of-Experts Language Model (MoE-LLM) for personalized explanation generation. It is designed to bridge the gap between recommendation reasoning and natural language explanations under both sparse and dense user interaction scenarios.

---

## ğŸš€ Highlights

- **Latent Clustering on the Unit Hypersphere**  
  Learns compact user-item latent representations using a vMF (von Mises-Fisher) mixture prior, enhanced with kNN-based perturbation and cosine consistency.

- **Cluster-Guided Explanation via MoE-LLM**  
  Uses cluster assignments to route user-item pairs to LoRA-finetuned lightweight LLM experts, enabling personalized and semantically faithful explanations.

- **Lightweight & Scalable**  
  Expert modules are efficient and modular, making training feasible under limited resources.

---

## ğŸ“‚ Project Structure


<pre><code> HyperXRec/ â”œâ”€â”€ models/ # Encoder, Decoder, MoE-LLM components â”œâ”€â”€ vmfmix/ # vMFMM clustering and perturbation â”œâ”€â”€ data/ # Amazon & TripAdvisor datasets â”œâ”€â”€ utils/ # Metrics, logging, losses â”œâ”€â”€ train.py # Entry point for training â”œâ”€â”€ inference.py # Entry point for inference â””â”€â”€ README.md # Youâ€™re reading it! </code></pre>

## âš™ï¸ Dependencies

- Python â‰¥ 3.11
- PyTorch â‰¥ 2.10
- HuggingFace Transformers
- scikit-learn, faiss, numpy, nltk
