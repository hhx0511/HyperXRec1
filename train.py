import sys
import os
import argparse
import random
import numpy as np
import torch 
import deepspeed
from model.vae_cluster import Vae_Cluster_Es
from transformers import AutoTokenizer#AutoTokenizerï¼šè‡ªåŠ¨åŠ è½½ä¸ä½ æŒ‡å®šçš„é¢„è®­ç»ƒæ¨¡å‹ï¼ˆå¦‚LLaMAï¼‰é…å¥—çš„åˆ†è¯å™¨ã€‚ç”¨äºå°†æ–‡æœ¬è½¬ä¸º token ID è¾“å…¥æ¨¡å‹ã€‚
from rich.console import Console#Consoleï¼šç”¨äºåœ¨ç»ˆç«¯ä¸­æ‰“å°å¸¦é¢œè‰²/æ ¼å¼çš„å†…å®¹ï¼Œå¢å¼ºå¯è¯»æ€§ï¼ˆå¦‚æ—¥å¿—ã€çŠ¶æ€æç¤ºï¼‰
from tqdm import tqdm#tqdmï¼šä¸ºå¾ªç¯æ·»åŠ è¿›åº¦æ¡æ˜¾ç¤ºï¼Œæå‡è®­ç»ƒå¯è§†åŒ–ä½“éªŒã€‚
from torch.optim.lr_scheduler import StepLR#StepLRï¼šPyTorch çš„å­¦ä¹ ç‡è°ƒåº¦å™¨ï¼ŒæŒ‰å›ºå®š epoch æ­¥é•¿å‡å°‘å­¦ä¹ ç‡ï¼ˆé˜²æ­¢è¿‡æ‹Ÿåˆæˆ–è®­ç»ƒåœæ»ï¼‰ã€‚
from torch.utils.data import DataLoader,Dataset
from utils.pepler_dataloader import Dataset_Rs_Pytorch,DataLoader_Rs
from collections import Counter
from transformers import AutoTokenizer,Trainer,TrainingArguments,DataCollatorWithPadding,EarlyStoppingCallback,DataCollatorForSeq2Seq#AutoTokenizerï¼šè‡ªåŠ¨åŠ è½½ä¸ä½ æŒ‡å®šçš„é¢„è®­ç»ƒæ¨¡å‹ï¼ˆå¦‚LLaMAï¼‰é…å¥—çš„åˆ†è¯å™¨ã€‚ç”¨äºå°†æ–‡æœ¬è½¬ä¸º token ID è¾“å…¥æ¨¡å‹ã€‚
# HF å®˜æ–¹é«˜é˜¶è®­ç»ƒå¾ªç¯ï¼ˆç”¨åœ¨ RecTrainer åŸºç±»ï¼‰# æ‰€æœ‰è¶…å‚é›†ä¸­ç®¡ç† + è‡ªåŠ¨ç”Ÿæˆ --help # DataCollatorWithPadding, å¯¹é½ä¸åŒé•¿åº¦è¾“å…¥ï¼Œå¸¸ç”¨äºåˆ†ç±»ä»»åŠ¡  #DataCollatorForSeq2Seq,  ç”Ÿæˆå¼ä»»åŠ¡ padding + label shift
from datasets import load_from_disk
from utils.utils import TorchDataset2HuggingfaceDataset,plot_latent,RecTrainer,save_gate_index#å°† PyTorch æ ¼å¼æ•°æ®è½¬æ¢ä¸º HuggingFace æ•°æ®é›†æ ¼å¼ã€‚å¯è§†åŒ– latent spaceï¼ˆVAE çš„ z ç©ºé—´ï¼‰èšç±»ç»“æ„ï¼Œè¾“å‡ºå›¾ç‰‡ã€‚è‡ªå®šä¹‰çš„ Trainer ç±»ï¼Œå°è£…è®­ç»ƒé€»è¾‘ï¼ˆå…¼å®¹ LoRA / å¤šä»»åŠ¡ç­‰ï¼‰ã€‚ä¿å­˜èšç±»åˆ†é…çš„ gate ç´¢å¼•ï¼ˆç”¨äºè§£é‡Šç”Ÿæˆæ¨¡å‹çš„é—¨æ§æœºåˆ¶ï¼‰ã€‚
from utils.prompt_process import Prompt_Process#æ•°æ®é¢„å¤„ç†å‡½æ•°ï¼Œå°†ç”¨æˆ·/ç‰©å“/è¯„åˆ†ä¿¡æ¯è½¬æ¢ä¸ºè‡ªç„¶è¯­è¨€è§£é‡Š promptï¼Œä¾‹å¦‚ï¼šâ€œUser X likes item Y with 4 stars becauseâ€¦â€
from peft import LoraConfig, TaskType, get_peft_model#LoraConfigä¸€ç§å‚æ•°é«˜æ•ˆå¾®è°ƒæŠ€æœ¯ï¼Œåªå¼•å…¥å°‘é‡å¯è®­ç»ƒå‚æ•°ï¼Œé¿å…å¾®è°ƒæ•´ä¸ªå¤§æ¨¡å‹ã€‚å®ƒé€šè¿‡æ’å…¥ä½ç§©çŸ©é˜µè¿‘ä¼¼åˆ°åŸå§‹æƒé‡ä¸­ï¼Œæ˜¾è‘—å‡å°‘è®¡ç®—å’Œå­˜å‚¨æˆæœ¬ã€‚
                                                     #TaskTypeè¿™æ˜¯ä¸€ä¸ªæšä¸¾ç±»ï¼Œç”¨äºå‘Šè¯‰ PEFT å½“å‰çš„æ¨¡å‹ä»»åŠ¡æ˜¯ä»€ä¹ˆã€‚
                                                     #get_peft_modelå°†åŸå§‹çš„é¢„è®­ç»ƒæ¨¡å‹ï¼ˆå¦‚ LLaMAï¼‰åŒ…è£…æˆå¸¦ LoRA æ’ä»¶çš„æ¨¡å‹ï¼Œç”¨äºåªå¾®è°ƒæŒ‡å®šæ¨¡å—ã€‚ åŠ¨æ€æ›¿æ¢ nn.Linear ä¸º LoRAâ€‘Linear
from model.moe_layer_llama import MoeBlock_RS#MoeBlock_RSï¼šé—¨æ§æ··åˆä¸“å®¶ï¼ˆMoEï¼‰ç»“æ„çš„å®ç°ï¼Œç”¨äºæ ¹æ®èšç±»ç»“æœé€‰æ‹©è§£é‡Šæ¨¡å‹çš„ä¸åŒâ€œä¸“å®¶â€ã€‚
from model.vamoe import Vmoe_llama3#Vmoe_llama3ï¼šæ•´ä¸ªâ€œè§£é‡Šç”Ÿæˆæ¨¡å—â€çš„æ¨¡å‹ç±»ï¼Œå°è£…äº† LLaMA ä¸»å¹² + é—¨æ§ MoE + ç”¨æˆ·/ç‰©å“åµŒå…¥ + Prompt å¤´éƒ¨ç­‰ã€‚

# vMF èšç±»åº“ï¼ˆä½œè€…è‡ªå®šä¹‰å®ç°ï¼‰
from vmfmix.vmf import VMFMixture
# æ•°æ®é›†ä¸æ¨¡å‹ç»„ä»¶
from dsvae.model import Decoder, VMFMM, Encoder

# 1. LoRAâ€‘LLaMA è§£é‡Šæ¨¡å‹è®­ç»ƒå‡½æ•°
def train(model, train_dataset, eval_dataset, tokenizer, epoch, checkpoint_dir, args):
    # ä½¿ç”¨è‡ªå®šä¹‰ RecTrainerï¼ˆç»§æ‰¿ HF Trainerï¼‰å¯è®°å½• gate æ¿€æ´»
    trainer = RecTrainer(
        model             = model,
        train_dataset     = train_dataset,  
        eval_dataset      = eval_dataset,
        tokenizer         = tokenizer,
        data_collator     = DataCollatorForSeq2Seq(#è¿™æ˜¯ Hugging Face æä¾›çš„ä¸€ä¸ªå‡½æ•°ï¼Œç”¨äºå¤„ç†ä¸å®šé•¿åºåˆ—çš„ padding å’Œ batch å¯¹é½ï¼Œå¸¸ç”¨äºç”Ÿæˆå¼ä»»åŠ¡
            tokenizer     = tokenizer,
            padding       = True,# ç”Ÿæˆæ¨¡å‹éœ€è¦ padï¼Œå¹¶ä¿æŒ labels å¯¹é½
        ),
        save_lora         = True,# æ§åˆ¶æ˜¯å¦ä¿å­˜ LoRA å¾®è°ƒç»“æœ
        args = TrainingArguments( #æ˜¯ Hugging Face å®˜æ–¹å®šä¹‰çš„ä¸€ä¸ªç±»ï¼Œç”¨æ¥æŒ‡å®šè®­ç»ƒè¶…å‚æ•°ã€‚

            output_dir                     = checkpoint_dir,#æ¨¡å‹å’Œæ£€æŸ¥ç‚¹ä¿å­˜ä½ç½®
            save_strategy                  = 'steps',#æ¯éš” save_steps æ­¥ä¿å­˜ä¸€æ¬¡æ¨¡å‹
            save_steps                     = 1000,
            per_device_train_batch_size    = 1,# çœŸæ­£ batch=1 â†’ é…åˆç´¯ç§¯,ç”¨å° batch(=1) èµ°å‰å‘ï¼ŒæŠŠæ¢¯åº¦æ”’å¤Ÿå†æ›´æ–°ï¼Œå¯â€œæ¨¡æ‹Ÿâ€æ›´å¤§æ‰¹æ¬¡ã€‚
            learning_rate                  = 3e-5,
            num_train_epochs               = epoch,#ç”±3æ”¹æˆäº†1ï¼Œæ…¢æ…¢éªŒè¯
            gradient_accumulation_steps    = 16,#16 æ­¥ç´¯ç§¯ä¸€æ¬¡æ¢¯åº¦ï¼Œç›¸å½“äºæ‰©å¤§ batch size,æŠŠ 16 ä¸ªå°æ‰¹æ¬¡çš„æ¢¯åº¦ç›¸åŠ ï¼Œå†åšä¸€æ¬¡ä¼˜åŒ–å™¨ step() æ›´æ–°å‚æ•°ã€‚
            # --------- logging arguments --------- #
            logging_strategy               = 'steps',
            logging_steps                  = 10,#	æ¯ 10 æ­¥è®°å½•ä¸€æ¬¡è®­ç»ƒæ—¥å¿—
            report_to                      = 'tensorboard',#è¾“å‡ºæ—¥å¿—åˆ° TensorBoard
            save_safetensors               = True,# å®‰å…¨ä¸”å¯çº¿ä¸ŠåŠ è½½

            max_grad_norm                  = 0.3,# é¿å…æ¢¯åº¦çˆ†ç‚¸
            gradient_checkpointing         = True,	#å¯ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹ï¼ŒèŠ‚çœæ˜¾å­˜
            deepspeed                      = args.deepspeed,  
            bf16                           = True  # æ–°å¢ï¼šå¼ºåˆ¶ä½¿ç”¨ bfloat16 ç²¾åº¦è®­ç»ƒ
        )
    )

    print(len(trainer.train_dataset['input_ids'][0]),len(trainer.train_dataset['labels'][0]))#æ‰“å°ç¬¬ä¸€ä¸ªæ ·æœ¬çš„è¾“å…¥å’Œæ ‡ç­¾é•¿åº¦ï¼Œå¸®åŠ©æ£€æŸ¥æ•°æ®æ˜¯å¦å¤„ç†æ­£ç¡®ã€‚
    print('start {} training!'.format(args.dataset))
    trainer.train()

    print('{} training done!'.format(args.dataset))

    # ====================== save model ===================== #
    # trainer.save_model(checkpoint_dir)
    print('{} model saved!'.format(args.dataset))

# 2. ä¸»å…¥å£ â€”â€” å…¨æµç¨‹    
console = Console()
if __name__ == '__main__':
     # ---------------- CLI å‚æ•° ----------------
    parser = argparse.ArgumentParser(description='VMoe_Rs')
     # æ•°æ®è·¯å¾„ä¸åç§°
    parser.add_argument('--dataset', type=str, default='Yelp',
                        help='dataset name, ex: Amazon, Yelp, TripAdvisor')  #"æ•°æ®é›†åç§°"   
    parser.add_argument('--data_path', type=str, default='/home/mail/2023t3/u430201701/hxproject/GavaMOE-vmf/datasets/Yelp/reviews.pickle',
                        help='data path') #"è¯„åˆ†æ•°æ®æ ¹ç›®å½•"           
    parser.add_argument('--index_dir', type=str, default='/home/mail/2023t3/u430201701/hxproject/GavaMOE-vmf/datasets/Yelp/1',
                        help='dataset index file')  #"ç”¨æˆ·/ç‰©å“ç´¢å¼•æ–‡ä»¶å¤¹"   
     # VAE é¢„è®­ç»ƒ / èšç±» å‚æ•°
    parser.add_argument('--pretrain_epochs', type=int, default= 300,#150
                        help='epoch of pretrain GMM')  #"VAE é¢„è®­ç»ƒ epoch"   
    parser.add_argument('--latent_dim', type=int, default = 16,#128æ”¹æˆ16
                        help='latent dim')    #     "éšç©ºé—´ç»´åº¦"
    parser.add_argument('--embedding_size', type=int, default = 768,
                        help='user-item embedding size')    #"ç”¨æˆ·/ç‰©å“åµŒå…¥ç»´åº¦"  
    parser.add_argument('--num_cluster', type=int, default = 3,
                        help='number of cluster')     
    parser.add_argument('--pretrain_model_path', type=str, default='/home/mail/2023t3/u430201701/hxproject/GavaMOE-vmf/meta-llama/Llama-3-8B-Instruct',
                        help='local path of llm')   #"LLaMAâ€‘3 æœ¬åœ°æƒé‡"
    parser.add_argument('--batch_size', type=int, default = 1024,
                        help='batch size') 
    parser.add_argument('--cuda', action='store_true',default=True,
                        help='use CUDA')#å¯ç”¨ CUDA
    parser.add_argument('--pretrain_weight_save', type = str, default='/home/mail/2023t3/u430201701/hxproject/Yelp_GavaMOE-vmf3/output/Yelp3',
                        help='path to save the pretraining model')#ä¿å­˜ VAE æƒé‡ä½ç½®
    parser.add_argument('--cluster_epoch', type=int, default = 30,#30
                        help='epoch of cluster')#èšç±»è®­ç»ƒ epoch
    parser.add_argument('--lr', type=int, default =  0.00001,#0.00001
                        help='Learning rate for training vae & gmm')#èšç±»é˜¶æ®µ Adam å­¦ä¹ ç‡
    parser.add_argument('--output_dir', type = str, default = '/home/mail/2023t3/u430201701/hxproject/Yelp_GavaMOE-vmf3/output/Yelp3',
                        help='Explainable Model Training Results Storage Path')#è§£é‡Šæ¨¡å‹è¾“å‡ºç›®å½•
    parser.add_argument('--llm_epoch', type = int, default = 3, help='epoch of llm')#LoRA è®­ç»ƒ epoch
    parser.add_argument('--local_rank', type=int, default=-1, help='Deepspeed will pass this automatically')
    parser.add_argument('--deepspeed', type=str, default=None, help='deepspeed config path')

    args = parser.parse_args()

    # ========================================================  Config Setting  ======================================================== 
    seed = 105
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    if torch.cuda.is_available():
        torch.backends.cudnn.deterministic = True
        torch.backends.cudnn.benchmark = False
        torch.cuda.manual_seed_all(seed)
        device = 'cuda'
    else:
        device = 'cpu'

     # ---------------- åˆ›å»ºè¾“å‡ºç›®å½• ----------------
    if not os.path.exists(os.path.join(args.pretrain_weight_save, args.dataset)):
        os.makedirs(os.path.join(args.pretrain_weight_save,args.dataset), exist_ok=True)
        console.print(f'{args.dataset} Will be Save {os.path.join(args.pretrain_weight_save, args.dataset)}')
    
    # ---------------- åŠ è½½åˆ†è¯å™¨ ----------------
    tokenizer = AutoTokenizer.from_pretrained(args.pretrain_model_path)
    tokenizer.pad_token = tokenizer.eos_token# # å› ä¸ºLM é‡åˆ° pad ä¼šå½“ä½œ eos

   # ---------------- åŠ è½½æ•°æ® ----------------
    console.print('Loading data...',style = 'bold green')
    max_text_length = 30 # prompt æˆªæ–­é˜²æ­¢è¶…é•¿
    
    corpus = DataLoader_Rs(args.data_path, args.index_dir, tokenizer, max_text_length)#self.train, self.valid, self.test, self.user2feature, self.item2feature 
    #self.user2featureè®°å½•äº†æ¯ä¸ªç”¨æˆ·åœ¨è®­ç»ƒé›†ä¸­æåˆ°è¿‡çš„ç‰¹å¾ï¼ˆfeatureï¼‰{user_index: [feature1, feature2, ...]}
    #åŒæ ·æ˜¯ä¸€ä¸ªå­—å…¸ï¼Œè®°å½•äº†æ¯ä¸ªç‰©å“è¢«ç”¨æˆ·æåŠçš„ç‰¹å¾åˆ—è¡¨ï¼Œä¹Ÿæ˜¯ç”¨äºå†·å¯åŠ¨æˆ–è§£é‡Šç”Ÿæˆ
    n_user = len(corpus.user_dict)#è¡¨ç¤ºç”¨æˆ·æˆ–ç‰©å“çš„ç§ç±»æ•°
    n_item = len(corpus.item_dict)
    
    # æ‰“å°å‚æ•°
    print('-' * 40 + 'ARGUMENTS' + '-' * 40)#----------------------------------------ARGUMENTS----------------------------------------
    for arg in vars(args):#éå†æ¯ä¸€ä¸ªå‚æ•°åç§°ï¼ˆkeyï¼‰
        console.print('{:40} {}'.format(arg, getattr(args, arg)))#getattr(args, arg)è·å–è¯¥å‚æ•°çš„å€¼,'{:40} {}'.format(...)æŠŠå‚æ•°åç§°å·¦å¯¹é½ï¼Œå  40 å­—ç¬¦å®½ï¼Œç„¶åæ‰“å°å¯¹åº”å€¼
    console.print(f"user num: {n_user} item num: {n_item}")#æ‰“å°ç”¨æˆ·å’Œç‰©å“æ•°é‡
    print('-' * 40 + 'ARGUMENTS' + '-' * 40)
    
    # ========================================================  Pretraining       ======================================================== 
    # ---------------- Phase 1ï¼šVAE+GMM é¢„è®­ç»ƒ ----------------
    #ä»¥VAE ä¼šæŠŠ ID æ˜ å°„æˆä¸€ä¸ªç¨ å¯†å‘é‡ï¼ˆæ¯”å¦‚ 768 ç»´ï¼‰ï¼Œè¿™äº›å‘é‡å°±åŒ…å«äº†ç”¨æˆ·/ç‰©å“çš„ååŒè¿‡æ»¤è¯­ä¹‰ï¼ˆè¡Œä¸ºã€åå¥½ç­‰ï¼‰ã€‚
    vae_clu = Vae_Cluster_Es(n_user = n_user,n_item = n_item,args = args)
    # ä¿å­˜æ¨¡å‹ç»“æ„ï¼Œæ–¹ä¾¿è°ƒå‚å¯¹æ¯”
    with open(os.path.join(args.pretrain_weight_save, args.dataset, args.dataset + '_output.txt'), 'w') as f:#æŠŠæ¨¡å‹ç»“æ„ï¼ˆvae_cluï¼‰å†™å…¥æ–‡æœ¬æ–‡ä»¶ï¼Œä¿å­˜ä¸‹æ¥
        f.write(str(vae_clu))#str(vae_clu) ä¼šè‡ªåŠ¨è°ƒç”¨ __str__() æ–¹æ³•ï¼ŒæŠŠæ¨¡å‹ç»“æ„ï¼ˆåŒ…æ‹¬å‚æ•°ã€å±‚ç»“æ„ç­‰ï¼‰è½¬æ¢æˆå­—ç¬¦ä¸²æ ¼å¼ã€‚
    vae_clu = vae_clu.to(device)
    vae_clu.pretrain(corpus = corpus, pretrain_epoch = args.pretrain_epochs)
    
    console.print(f'Pretraining finished....')#VAE é¢„è®­ç»ƒå®Œæˆ
    # ========================================================  Cluster Training  ======================================================== 
    console.print(f'Cluster Training...')
    # vae_clu.cluster_training(corpus = corpus, cluster_epoch = 100)

     # ---------------- Phase 2ï¼šèšç±»ç²¾è°ƒ --------------------
    console.print(f'Start Cluster Training......', style='bold red')#èšç±»å¾®è°ƒä¸­â€¦
    cluster_epoch = args.cluster_epoch  # è¯»å– CLI è¶…å‚ï¼›å¯åœ¨å‘½ä»¤è¡Œ --cluster_epoch è°ƒå¤§/ç¼©çŸ­ï¼Œ30
    epoch_bar = tqdm(range(cluster_epoch)) # tqdm è¿›åº¦æ¡ï¼Œå®æ—¶æ˜¾ç¤º epoch è¿›åº¦
    data_loader = DataLoader(Dataset_Rs_Pytorch(corpus.train),batch_size = args.batch_size, shuffle = True)# DataLoaderï¼šåŒ…ä¸€å±‚ Dataset_Rs_Pytorchï¼ŒæŠŠåŸå§‹åˆ—è¡¨è½¬ä¸ºå¼ é‡ï¼›æŒ‰ args.batch_size éšæœºæ‰“ä¹±
    losses = [] 
    accuracies = []                                                                                           # user, item, rating , text, feature                                                      
    # lr=0.001 better   lr is important,2e-3 lead to posterior collapse ğŸ¤¡
    optimizer = torch.optim.Adam(vae_clu.parameters(),lr = args.lr)#0.00001
    
    lr_s = StepLR(optimizer, step_size = 10, gamma = 0.5)# StepLRï¼šæ¯ 10 epoch æŠŠ lr ä¹˜ 0.5 â€”â€”> â€œé˜¶æ¢¯è¡°å‡â€
    print(f'len dataloader: {len(data_loader)}')# æŸ¥çœ‹ä¸€å…±å¤šå°‘ batchï¼Œä¾¿äºä¼°ç®—æ˜¾å­˜/æ—¶é—´

    scale_factor_kl = 0.01 #Î²â€‘VAEï¼šå…ˆå°åå¤§ï¼Œé˜²æ­¢ posterior collapse; # Î²-VAE æ€æƒ³ï¼šå…ˆè®© KL ç³»æ•°å°ï¼Œèšç„¦é‡æ„ï¼›å†é€æ­¥â†‘ é¿å… collapse
    kl_increase = True# æ˜¯å¦å¯ç”¨ KL é€€ç«
    best_val_loss = float('inf')
    for epoch in epoch_bar:
        # lr_s.step()
        epoch = epoch + 1
        loss_all = 0
        acccuracy_all=0
        losses_epoch = 0.
        accuracies_epoch = 0.
        #print(f'scale_factor_kl is {scale_factor_kl}')
        for batch_index,(user, item, rating, _, _) in enumerate(data_loader):
            user = user.to(device)
            item = item.to(device)
            rating = rating - 1
            rating = rating.to(device)
            # compute elbo loss -> batch loss
            loss = vae_clu.vmfmm_Loss(user, item, rating, scale_factor_kl)# Lâ€² = 0.1 Ã— Lï¼Œæ¢¯åº¦å˜æˆ 0.1 Ã— âˆ‚L/âˆ‚Î¸ã€‚æ‰¹å¤§å° 4 K + æ¢¯åº¦ç´¯ç§¯ 16	æœ‰æ•ˆ batch â‰ˆ 64 Kï¼Œæ¢¯åº¦å¤©ç„¶æ›´å¤§ï¼›å†é…å¤§ loss ç³»æ•°ï¼Œçˆ†æ˜¾å­˜æˆ–æ•°å€¼æº¢å‡º
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()
            loss_all += loss
            # ---- å†è®¡ç®—å‡†ç¡®ç‡ï¼ˆä¸å æ¢¯åº¦å›¾ï¼‰
            with torch.no_grad():
                acccuracy = vae_clu.vmfmm_accuracy(user, item, rating)
            acccuracy_all += acccuracy
        # æ¯ 5 epoch å¢å¤§ KL ç³»æ•° + å¯è§†åŒ– latent
        if epoch % 5 == 0: # scale_factor_kl 0.2 is better than 0.3
            print('scale up scale_factor_kl')
            if kl_increase:#kl_increase = True# æ˜¯å¦å¯ç”¨ KL é€€ç«
                scale_factor_kl += 0.005
                if scale_factor_kl >= 0.1:
                    scale_factor_kl = 0.1  
        plot_latent(vae_clu, data_loader, args, epoch)
        # è®¡ç®—å½“å‰ epoch çš„å¹³å‡ lossï¼ˆè™½ç„¶è¿™é‡Œæ²¡ç”¨åˆ°ï¼Œç”¨äº early-stop æˆ–æ‰“å°ï¼‰
        losses_epoch = loss_all.item() / len(data_loader)  #losses_epoch
        accuracies_epoch =  acccuracy_all  / len(data_loader)
        # ä¿å­˜æœ€ä½³æƒé‡ï¼ˆè¿™é‡Œç”¨ train loss å½“ proxyï¼‰
        if losses_epoch < best_val_loss:      
            best_val_loss = losses_epoch
            torch.save(vae_clu.state_dict(), os.path.join(args.pretrain_weight_save, args.dataset, args.dataset + '_' +f'cluster_{args.num_cluster}_best_weight.pth'))
            print(f'Saving Best Pretraining Model for loss {best_val_loss}')

        lr_s.step()
        print(f'Epoch {epoch} Loss: {loss_all.item() / len(data_loader)}')
        print(f'Epoch {epoch} Accuracy: {accuracies_epoch}')
        losses.append(loss_all.item() / len(data_loader))
        accuracies.append(accuracies_epoch)
        

        vae_clu.plot_loss_curve(losses, title=f'Cluster Training Loss Curve for {args.dataset}',save_path= os.path.join(args.pretrain_weight_save,args.dataset, args.dataset +'_'+ f'loss_cluster_{args.num_cluster}.png'))
        vae_clu.plot_accuracy_curve(accuracies, title=f'Cluster Training Accuracy Curve for {args.dataset}',save_path= os.path.join(args.pretrain_weight_save,args.dataset, args.dataset +'_'+ f'accuracy_cluster_{args.num_cluster}.png'))

        # æ¯ä¸ª epoch ä¹Ÿä¿å­˜ä¸€æ¬¡ï¼ˆå†—ä½™ä½†ç¨³å¦¥ï¼‰
        torch.save(vae_clu.state_dict(), os.path.join(args.pretrain_weight_save, args.dataset, args.dataset + '_' +f'_cluster_{args.num_cluster}_epoch_{epoch}.pth'))
    console.print(f'Explaination Generate Training Start......',style = 'bold green')
    # WHY: èšç±»å¾®è°ƒå®Œæˆ â†’ è¿›å…¥ â€œè§£é‡Šç”Ÿæˆâ€ é˜¶æ®µï¼ˆLoRA-LLaMA è®­ç»ƒï¼‰
    # ========================================================  Explaination Generate Training  ======================================================== 
    # construct Huggingface Dataset(æŠŠä½ ä¹‹å‰çš„æ•°æ®ï¼ˆå¦‚ corpus.trainï¼‰ä» Python åˆ—è¡¨ï¼ˆList[Dict]ï¼‰æ ¼å¼ â†’ è½¬ä¸º HuggingFace æ”¯æŒçš„ Dataset å¯¹è±¡æ ¼å¼ã€‚)
    # ================= Phase 3â€‘Aï¼šæ„å»º HF Dataset =======================
     # è®¾ç½®ç¼“å­˜è·¯å¾„ï¼Œé¿å…æ¯æ¬¡éƒ½é‡æ–° Tokenize â†’ æé«˜è°ƒè¯• & è®­ç»ƒæ•ˆç‡
    cache_dir = os.path.join("/home/mail/2023t3/u430201701/hxproject/GavaMOE-vmf/datasets", "cached_datasets",args.dataset)  # ç›®å½•ï¼šoutput_dir/cached_datasets/
    os.makedirs(cache_dir, exist_ok=True)                         # è‹¥ç›®å½•ä¸å­˜åœ¨åˆ™åˆ›å»º
    train_cache = os.path.join(cache_dir, "train")                # è®­ç»ƒé›†ç¼“å­˜ç›®å½•
    eval_cache = os.path.join(cache_dir, "eval")                  # éªŒè¯é›†ç¼“å­˜ç›®å½•
    test_cache = os.path.join(cache_dir, "test")                  # æµ‹è¯•é›†ç¼“å­˜ç›®å½•

    # æ˜¯å¦ä½¿ç”¨ç¼“å­˜æœºåˆ¶ï¼Œé»˜è®¤å¼€å¯ï¼ˆç”± argparse å‚æ•°æ§åˆ¶ï¼‰
    use_cache = True if hasattr(args, "use_cache") and args.use_cache else False

    # ---------------------- è‹¥å­˜åœ¨ç¼“å­˜ï¼Œç›´æ¥åŠ è½½ ----------------------
    if use_cache and os.path.exists(train_cache):
        console.print("[green]Loading tokenized datasets from cache...[/green]")  # æç¤ºä¿¡æ¯
        train_dataset = load_from_disk(train_cache)  # ä»ç£ç›˜æ¢å¤è®­ç»ƒé›†
        eval_dataset = load_from_disk(eval_cache)    # éªŒè¯é›†
        test_dataset = load_from_disk(test_cache)    # æµ‹è¯•é›†

    # ---------------------- å¦åˆ™é‡æ–°å¤„ç† & ä¿å­˜ ----------------------
    else:
        console.print("[yellow]Processing raw data and saving to disk...[/yellow]")
    # Step 1ï¼šå°† PyTorch List[Dict] æ•°æ®ç»“æ„è½¬ä¸º HuggingFace Dataset å¯¹è±¡
    train_dataset = TorchDataset2HuggingfaceDataset(corpus.train, cache_dir='')
    eval_dataset  = TorchDataset2HuggingfaceDataset(corpus.valid, cache_dir='')
    test_dataset  = TorchDataset2HuggingfaceDataset(corpus.test,  cache_dir='')
    # WHYï¼šHuggingFace Trainer / map / filter / Dataloader éƒ½ä¾èµ– Dataset ç±»ã€‚
    # å…ˆç»Ÿä¸€æ ¼å¼ï¼Œåé¢çš„ mapã€shuffleã€batch å…¨å…è´¹è·å¾—

    # Step 2ï¼šå¯¹æ¯æ¡æ ·æœ¬è¿›è¡Œ Prompt æ„é€  + Tokenizeï¼ˆæ³¨æ„ batched=Falseï¼‰
    # Mapping the dataset 
    # bound to set batched to False, data process is not batched ref: prompt_precess.py examples['rating'] >=3 positive
    # -----------------------------------------------------------------------------
    # å¯¹æ¯æ¡è®°å½•åš Prompt é‡å†™ + Tokenize
    # -----------------------------------------------------------------------------
    print('Load the hf dataset...')#è¿™ä¸ª map æ“ä½œä¼š æ·»åŠ æ–°çš„å­—æ®µï¼ˆinput_ids, attention_mask, labelsï¼‰ï¼Œä½†å®ƒä¸ä¼šåˆ é™¤åŸå§‹å­—æ®µuser, item, rating , text, feature
    train_dataset = train_dataset.map( # æŠŠå‡½æ•°åº”ç”¨åˆ°æ¯æ¡æ ·æœ¬
        Prompt_Process(tokenizer, 180),# âœ æ„é€ è‡ªç„¶è¯­è¨€ prompt å¹¶ç¼–ç æˆ ID
        batched = False,               # âš ï¸ é€æ¡å¤„ç†ï¼Œå› å‡½æ•°é‡Œæœ‰ if/else
    )
    eval_dataset  = eval_dataset.map(
        Prompt_Process(tokenizer, 180),
        batched = False
    )
    test_dataset  = test_dataset.map(
        Prompt_Process(tokenizer, 180),
        batched = False
    )
    
    # WHYï¼š
    # 1. Prompt_Process æŠŠ (user,item,rating,text) å˜æˆ
    #    "User <u> likes Item <i> with 4 stars because â€¦"
    #    åŒæ—¶ç”Ÿæˆ input_ids / labelsï¼ˆå³ç§»ä¸€æ ¼ï¼‰
    # 2. max_len=180ï¼šæˆªæ–­è¿‡é•¿æ–‡æœ¬ï¼Œé˜²æ­¢ GPU OOM
    # 3. ä¸åš batched=Trueï¼šå‡½æ•°å†…éƒ¨æ ¹æ® rating æ­£è´Ÿå†™ä¸åŒæ¨¡æ¿

    # Step 3ï¼šä¿å­˜è‡³æœ¬åœ°ç£ç›˜ï¼Œä¸‹æ¬¡å¯ç›´æ¥ load_from_disk åŠ å¿«æµç¨‹
    train_dataset.save_to_disk(train_cache)
    eval_dataset.save_to_disk(eval_cache)
    test_dataset.save_to_disk(test_cache)
    # è§£ç é¦–æ¡æ ·æœ¬ï¼Œè‚‰çœ¼æ£€æŸ¥ Prompt æ˜¯å¦æ­£ç¡®
    console.print(tokenizer.decode(train_dataset['input_ids'][0]),style='bold green')#ä¸€æ¡è®­ç»ƒæ ·æœ¬çš„ input_ids,decode() ä¼šæŠŠ input_ids è½¬æ¢ä¸º äººç±»å¯è¯»çš„æ–‡æœ¬ï¼Œ
    # ç”± VAE é¢„æµ‹ (user,item) çš„èšç±» â†’ ä¿å­˜ä¸º gate ç´¢å¼•ï¼Œä¾› MoE è·¯ç”±
    #train_dataset = train_dataset.select(range(32))   # åªä¿ç•™å‰ 32 æ¡æ ·æœ¬
    

    # ç”± VAE é¢„æµ‹ (user,item) çš„èšç±» â†’ ä¿å­˜ä¸º gate ç´¢å¼•ï¼Œä¾› MoE è·¯ç”±
    train_cluster_index = save_gate_index(train_dataset, vae_clu)# é—¨æ§ç´¢å¼•,ä¸“å®¶è·¯ç”±ï¼šæ¨èè§£é‡Šæ¨¡å‹æ˜¯ä¸€ä¸ªé—¨æ§ MoEï¼ˆMixture of Expertsï¼‰ç»“æ„ï¼Œgate index æŒ‡ç¤ºæ¯æ¡æ ·æœ¬åº”è¯¥äº¤ç»™å“ªä¸ªä¸“å®¶æ¨¡å‹å¤„ç†ã€‚
    # WHYï¼šMoE æ¯ä¸ªâ€œä¸“å®¶â€å¯¹åº”ä¸€ä¸ªèšç±»ï¼›æå‰è®¡ç®—å¥½ç´¢å¼•ï¼Œè®­ç»ƒæ—¶ O(1) æŸ¥è¯¢ã€‚
    print(len(train_dataset['input_ids'][0]),len(train_dataset['input_ids'][1]))# double-check é•¿åº¦ä¸€è‡´
    # =============================================================================
    # Phase 3-Bï¼šæ„å»º LoRA-å¢å¼ºçš„ LLaMA-3 MoE æ¨¡å‹
    # =============================================================================
    # ---------- 1) é…ç½® LoRA ----------
    lora_config = LoraConfig(
        task_type = TaskType.CAUSAL_LM, ## å› æœè¯­è¨€æ¨¡å‹
        target_modules = ['q_proj','v_proj','k_proj','o_proj','user_embed','item_embed'],#å®ƒå‘Šè¯‰ LoRA åªå¯¹è¿™å‡ ä¸ªæ¨¡å—åŠ ä½ç§©å¯è®­ç»ƒå‚æ•°ï¼ˆLoRA æƒé‡ï¼‰ï¼Œè€Œä¸æ˜¯å¯¹æ•´ä¸ªæ¨¡å‹éƒ½å¾®è°ƒï¼Œå‡å°‘è®­ç»ƒèµ„æºæ¶ˆè€—ã€‚
        modules_to_save = ['f3','f1','f2',               #MoE ä¸“å®¶ç½‘ç»œ	ä¸åŒèšç±»çš„æ¨èè§£é‡Š
        'gate0','gate1','gate2',       #è·¯ç”±å™¨ï¼ˆé—¨æ§ï¼‰	æ ¹æ®è¾“å…¥å†³å®šèµ°å“ªä¸ªä¸“å®¶
        'user_proj','item_proj'],                        # æŠ•å½±æ¨¡å—	å°†ç”¨æˆ·/ç‰©å“åµŒå…¥æ˜ å°„åˆ°æ¨¡å‹å†…éƒ¨ç©ºé—´
        inference_mode = False,                          # è®­ç»ƒé˜¶æ®µ
        #åœ¨ LoRA ä¸­ï¼Œæˆ‘ä»¬å†»ç»“åŸå§‹å¤§æ¨¡å‹çš„å‚æ•°ï¼ˆæ¯”å¦‚ q_projã€v_proj ç­‰çŸ©é˜µï¼‰ï¼Œåªè®­ç»ƒä½ç§©çŸ©é˜µ A å’Œ Bï¼šW â€²=W+Î±â‹…Aâ‹…B,Wï¼šåŸå§‹é¢„è®­ç»ƒçš„æƒé‡ï¼ˆä¸åŠ¨ï¼‰,Aâ‹…Bï¼šä½ç§©çš„å¯å­¦ä¹ å‚æ•°ï¼ˆLoRA),Î±ï¼šä¸€ä¸ªç¼©æ”¾å› å­
        r = 8,                                           # LoRA rank=8
        lora_alpha = 16,                                 #LoRA å®é™…ä½œç”¨æ˜¯ï¼šæƒé‡å˜æˆ W + Î±ABï¼Œå…¶ä¸­ Î± = 16
        lora_dropout = 0.1                               #ç”¨æ¥å¢åŠ è®­ç»ƒæ—¶çš„éšæœºæ€§ï¼Œé˜²æ­¢è¿‡æ‹Ÿåˆ
)
    # ---------- 2) è½½å…¥ LLaMA-3 åŸºç¡€é…ç½® ----------
    from model.config_llama3 import llama_config
    
    config = llama_config
    print(config)# æ‰“å°ç¡®è®¤ vocab_size / n_layer / n_head ç­‰
    # ---------- 3) å– VAE è®­ç»ƒå¥½çš„ user/item åµŒå…¥ ----------
    user_embeds = vae_clu.encoder.user_embeddings
    item_embeds = vae_clu.encoder.item_embeddings

    # WHYï¼šæŠŠâ€œè¯„åˆ†é‡æ„â€é˜¶æ®µå­¦åˆ°çš„ ID è¡¨ç¤ºç›´æ¥æ¬è¿› LLMï¼Œ
    #      è®©æ–‡æœ¬è§£é‡Šæ¨¡å‹å¤©ç„¶å¸¦æœ‰ååŒè¿‡æ»¤ä¿¡æ¯ã€‚
    # è½¬ bfloat16ï¼šèŠ‚çœ 50% æ˜¾å­˜ï¼Œæ¨ç†/è®­ç»ƒæ›´å¿«ï¼›A100/H100 åŸç”Ÿæ”¯æŒ
    user_embeds = user_embeds.to(torch.bfloat16)
    item_embeds = item_embeds.to(torch.bfloat16)
    # ---------- 4) æ„é€  Vmoe_llama3 ä¸»å¹² ----------
    vmoe_llama3 = Vmoe_llama3(config = config,                       # åŸºç¡€ GPT æ¡†æ¶
                              tokenizer = tokenizer,                 # è¯è¡¨ â†’ æ–¹ä¾¿è‡ªåŠ¨ resize
                              gate_index_list = train_cluster_index, # æ¯æ¡æ ·æœ¬è·¯ç”±åˆ°å“ªä¸ªä¸“å®¶
                              user_embed = user_embeds,              # å†»ç»“åçš„åµŒå…¥æƒé‡
                              item_embed = item_embeds, 
                              use_lora = False)                      # å†»ç»“ baseï¼ŒLoRA è¦†ç›–, # å…ˆä¸ç»™ base æ³¨å…¥ LoRAï¼Œäº¤ç»™ PEFT å¤„ç†
    
    # ---------- 5) ç”¨ PEFT åŒ…è£…ï¼ŒåŠ ä¸Š LoRA Adapter ----------
    model_llama3 = get_peft_model(vmoe_llama3,lora_config)
    # é‡Šæ”¾ VAE å ç”¨çš„æ˜¾å­˜
    vae_clu = vae_clu.to('cpu') #â‘  æŠŠ VAE æ¨¡å‹æ•´ä½“æ¬åˆ° CPUï¼›vae_clu è¢«ç§»åˆ° CPU å¹¶åˆ é™¤ â†’ å®ƒçš„å‚æ•°ä¸å†å‚ä¸åå‘ä¼ æ’­ï¼ŒL_ELBO å³ä½¿ç®—å‡ºæ¥ä¹Ÿå¯¹æ¢¯åº¦æ— è´¡çŒ®ã€‚
    del vae_clu# â‘¡ åˆ é™¤ VAE å¯¹è±¡ï¼Œå½»åº•æ–­å¼€è®¡ç®—å›¾
    torch.cuda.empty_cache()# â‘¢ æ¸…ç† GPU æ˜¾å­˜ç¼“å­˜
    
    print('Already Freeze the user item embedding...')
    # æ‰“å°å¯è®­ç»ƒå‚æ•°æ¯”ä¾‹ï¼Œç¡®è®¤ LoRA ç”Ÿæ•ˆ
    print(model_llama3.print_trainable_parameters())
    
    # ================= Phase 3â€‘Cï¼šLoRA è®­ç»ƒ =============================
    #RecTrainer ç»§æ‰¿äº† HuggingFace Trainerï¼Œé»˜è®¤æŠŠ outputs.loss å½“ä½œæ€»æŸå¤±åš backward()ï¼Œäºæ˜¯ Stage-2 è®­ç»ƒ åªå¯¹ LM Loss æ›´æ–° LoRA å’Œ MoE å‚æ•°ã€‚
    explain_checkpoint_dir = args.output_dir + '/explain'
    import torch.distributed as dist

    # è®¾å®šå½“å‰å¡
    if dist.is_available() and dist.is_initialized():
        local_rank = int(os.environ.get("LOCAL_RANK", args.local_rank))
        torch.cuda.set_device(local_rank)
    deepspeed.utils.set_z3_leaf_modules(model_llama3, [MoeBlock_RS])

    # rank 0 æ‰“å°ä¿¡æ¯
    if not dist.is_initialized() or dist.get_rank() == 0:
        print("ğŸš€ Starting training on rank:", dist.get_rank() if dist.is_initialized() else 0)

    # æ‰€æœ‰è¿›ç¨‹å¡ä¸€èµ·ç­‰å¾… â†’ åŒæ­¥è¿›å…¥è®­ç»ƒ
    if dist.is_initialized():
        dist.barrier()
    train(  
            epoch                   = args.llm_epoch, 
            model                   = model_llama3, 
            tokenizer               = tokenizer,
            train_dataset           = train_dataset,
            eval_dataset            = None,
            checkpoint_dir          = explain_checkpoint_dir,
            args                    = args
    )
    #model_llama3.save_pretrained(explain_checkpoint_dir)
    #print('Saved Model... && Training Done...')
    # è®­ç»ƒåï¼Œä»… rank 0 ä¿å­˜æ¨¡å‹
    if not dist.is_initialized() or dist.get_rank() == 0:
        model_llama3.save_pretrained(explain_checkpoint_dir)
        print('Saved Model... && Training Done...')

